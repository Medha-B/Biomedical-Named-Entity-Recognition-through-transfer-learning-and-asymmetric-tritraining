{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook for data preprocessing. \n",
        "\n",
        "Reference: Gao S, Kotevska O, Sorokine A, Christian JB (2021) A pre-training and self-training approach for biomedical named entity recognition. PLoS ONE 16(2): e0246310. https://doi.org/10.1371/journal.pone.0246310\n",
        "\n",
        "Code: https://code.ornl.gov/biomedner/biomedner"
      ],
      "metadata": {
        "id": "AG1nHQWLbZte"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for MedMentions Dataset"
      ],
      "metadata": {
        "id": "pkMkQtk97-b0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur2xkspOwWLt",
        "outputId": "9a3d9f85-bed4-49cf-aab2-7cfe78837572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▋                           | 10 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 20 kB 20.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 30 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 40 kB 10.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 51 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71 kB 3.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pysbd\n",
            "Successfully installed pysbd-0.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install pysbd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTURmYhJzi11"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import gzip\n",
        "import pickle\n",
        "import pysbd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKHxkKUEpd_k",
        "outputId": "0d59d785-222a-4907-f4b3-678b483d5b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s55OosJfzojm"
      },
      "outputs": [],
      "source": [
        "medmentions_gz ='/content/drive/MyDrive/corpus_pubtator.txt.gz'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6kUKQLB11BJd",
        "outputId": "13cb43a1-f061-41fa-d1d0-8557c46e666c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/corpus_pubtator.txt.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "medmentions_gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-GDurKtzsX_"
      },
      "outputs": [],
      "source": [
        "seg = pysbd.Segmenter(language=\"en\", clean=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1WCQyuQ2dw7"
      },
      "outputs": [],
      "source": [
        "def read(ifile):\n",
        "    obj = {\"mention\": []}\n",
        "\n",
        "    HEADER = re.compile(r\"(?P<pmid>[0-9]*)\\|(?P<t>[t|a])\\|(?P<content>.*)\")\n",
        "    MENTIONS = re.compile(r\"(?P<pmid>[0-9]*)\\t(?P<start>[0-9]*)\\t(?P<end>[0-9]*)\\t(?P<content>.*)\\t(?P<tui>(T.+|UnknownType))\\t(?P<cui>C[0-9]+)\")\n",
        "\n",
        "    with gzip.open(ifile, 'r') as fin:\n",
        "        for line in fin:\n",
        "            l = line.decode(\"utf-8\")\n",
        "            h = HEADER.match(l)\n",
        "            if h:\n",
        "                obj[\"pmid\"] = int(h.group(\"pmid\"))\n",
        "                obj[h.group(\"t\")] = h.group(\"content\")\n",
        "                continue\n",
        "            m = MENTIONS.match(l)\n",
        "            if m:\n",
        "                mention = {\"start\": m.group(\"start\"),\n",
        "                           \"end\": m.group(\"end\"),\n",
        "                           \"content\": m.group(\"content\"),\n",
        "                           \"tui\": m.group(\"tui\").split(\",\"),\n",
        "                           \"cui\": m.group(\"cui\")}\n",
        "                obj[\"mention\"].append(mention)\n",
        "                continue\n",
        "            else:\n",
        "                yield obj\n",
        "                obj = {\"mention\": []}\n",
        "              \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78eEcYFY10ua"
      },
      "outputs": [],
      "source": [
        "# helper function for separating non-alphanumeric characters\n",
        "def add_space(text):\n",
        "    out = []\n",
        "    for c in text:\n",
        "        if c.isalnum():\n",
        "            out.append(c)\n",
        "        elif c == ' ':\n",
        "            out.append(c)\n",
        "        else:\n",
        "            out.extend([' ',c,' '])\n",
        "    return ''.join(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0i7EHC01IXO"
      },
      "outputs": [],
      "source": [
        "# save all data to a dictionary\n",
        "data = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYP7C8OS2GQV"
      },
      "outputs": [],
      "source": [
        "# iterate through medmentions gzip file\n",
        "for i,obj in enumerate(read(medmentions_gz)):\n",
        "\n",
        "    # load abstract\n",
        "    title = obj['t']\n",
        "    pmid = obj['pmid']\n",
        "    l = len(title) + 1\n",
        "    abstract = obj['a']\n",
        "\n",
        "    # label codes for each character in the abstract\n",
        "    filler = ['0' if c.isalnum() else c for c in abstract]\n",
        "\n",
        "    # keep track of all named entities\n",
        "    spans = []\n",
        "    \n",
        "    # iterate through each named entity\n",
        "    for mention in obj[\"mention\"]:\n",
        "        start = int(mention['start']) - l\n",
        "        end = int(mention['end']) - l\n",
        "        if start < 0:\n",
        "            continue\n",
        "        span = abstract[start:end]\n",
        "        spans.append(span)\n",
        "        \n",
        "        # generate a label code for each character in the named entity\n",
        "        # this is necessary because named entities with non-alphanumerics will confuse\n",
        "        # the BERT tokenizer later\n",
        "        codes = []\n",
        "        for c in span:\n",
        "            if c.isalnum():\n",
        "                codes.append('1')\n",
        "            elif c == ' ':\n",
        "                codes.append(' ')\n",
        "            else:\n",
        "                codes.append('¢')\n",
        "                \n",
        "        # ignore non-alphanumerics at the beginning of a named entity \n",
        "        for i,c in enumerate(codes):\n",
        "            if c == '¢':\n",
        "                codes[i] = '#'\n",
        "            else:\n",
        "                codes[i] = '2'\n",
        "                break\n",
        "                \n",
        "        # update label codes for named entity span in abstract\n",
        "        filler[start:end] = codes\n",
        "        \n",
        "    # convert label codes into pseudo text\n",
        "    filler = ''.join(filler)\n",
        "    filler = add_space(filler)\n",
        "    abstract = add_space(abstract)\n",
        "    abstract = abstract.split()\n",
        "    filler = filler.split()\n",
        "    \n",
        "    # convert pseudo text to B,I,O NER labels\n",
        "    labels = []\n",
        "    for w in filler:\n",
        "        if w[0] == '0':\n",
        "            labels.append('O')\n",
        "        elif w[0] == '1':\n",
        "            labels.append('I')\n",
        "        elif w[0] == '2':\n",
        "            labels.append('B')\n",
        "        elif w[0] == '¢':\n",
        "            labels.append('I')\n",
        "        else:\n",
        "            labels.append('O')\n",
        "\n",
        "    # track any mismatches for debugging\n",
        "    if len(labels) != len(abstract):\n",
        "        print(labels,abstract,filler)\n",
        "        print(len(labels),len(abstract),len(filler))\n",
        "    \n",
        "    # sentence boundary detection to convert abstract into individual sentence inputs\n",
        "    abstract = ' '.join(abstract)\n",
        "    sentences = [s.split() for s in seg.segment(abstract)]\n",
        "    sent_labels = []\n",
        "    i = 0\n",
        "    for s in sentences:\n",
        "        l = len(s)\n",
        "        sent_labels.append(labels[i:i+l])\n",
        "        i += l\n",
        "    \n",
        "    # add cleaned text and NER labels to dictionary\n",
        "    for i,(s,l) in enumerate(zip(sentences,sent_labels)):\n",
        "        sample = {}\n",
        "        sample['sentence'] = s\n",
        "        sample['labels'] = l\n",
        "        uid = str(pmid) + \"_\" + str(i)\n",
        "        data[uid] = sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdpvdOWo2K2E"
      },
      "outputs": [],
      "source": [
        "# save to disk\n",
        "file_name = '/content/drive/MyDrive/medmentions_ner.pkl'\n",
        "with open(file_name,'wb') as f:\n",
        "    pickle.dump(data,f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing Step for PubMed Word Embeddings"
      ],
      "metadata": {
        "id": "Jv2mu_Oe7tez"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtfiBJwK2OJn",
        "outputId": "24d466d6-57f1-4fa3-909a-4c8c426934d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim==3.8.3 in /usr/local/lib/python3.7/dist-packages (3.8.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.3) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim==3.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sozJbAw42m9R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "import logging\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz0fEjK4zxcY"
      },
      "outputs": [],
      "source": [
        "pubmed_w2v = '/content/drive/MyDrive/PubMed-w2v.bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_ZQX_X_4vOn"
      },
      "outputs": [],
      "source": [
        "#logging setup\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
        "                    level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NW81D2dB414t"
      },
      "outputs": [],
      "source": [
        "#load data\n",
        "file_name = '/content/drive/MyDrive/medmentions_ner.pkl'\n",
        "with open(file_name,'rb') as f:\n",
        "    data = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDfhbZ7Dy_G_"
      },
      "outputs": [],
      "source": [
        "#extract data\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for i,(k,v) in enumerate(data.items()):\n",
        "    sentences.append(v['sentence'])\n",
        "    labels.append(v['labels'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjgG7CVzzB-J",
        "outputId": "4132f48b-53fa-4470-b75d-81d30bcb0fab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17.26698073028836 24.946385920804527 483 82.0\n"
          ]
        }
      ],
      "source": [
        "#find max, mean, and std length of sentences\n",
        "sentence_lens = [len(s) for s in sentences]\n",
        "sen_std = np.std(sentence_lens)                         \n",
        "sen_mean = np.mean(sentence_lens)                       \n",
        "sen_max = np.amax(sentence_lens)                        \n",
        "sen_precentile = np.percentile(sentence_lens,99)        \n",
        "print(sen_std,sen_mean,sen_max,sen_precentile)\n",
        "max_len = 100 \n",
        "sentence_lens = [i if i < max_len else max_len for i in sentence_lens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO0OGRu7zK16"
      },
      "outputs": [],
      "source": [
        "#convert labels into integers\n",
        "labels_notype = np.ones((len(labels),max_len)) * -1\n",
        "for i,label in enumerate(labels):\n",
        "\n",
        "    #convert to label indices without type\n",
        "    notype = []\n",
        "    for l in label[:max_len]:\n",
        "        if l == 'O':\n",
        "            notype.append(0)\n",
        "        elif l == 'B':\n",
        "            notype.append(1)\n",
        "        elif l == 'I':\n",
        "            notype.append(2)\n",
        "        else:\n",
        "            raise Exception('invalid code')\n",
        "\n",
        "    l = len(notype)\n",
        "    labels_notype[i,:l] = notype\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOxJcFvyzPJ0",
        "outputId": "f1bd30e8-1c87-4485-fba0-45aa8bd93ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-10 06:24:07,488 : INFO : loading projection weights from /content/drive/MyDrive/PubMed-w2v.bin\n",
            "2022-04-10 06:24:47,420 : INFO : loaded (2351706, 200) matrix from /content/drive/MyDrive/PubMed-w2v.bin\n"
          ]
        }
      ],
      "source": [
        "#load word2vec\n",
        "model = KeyedVectors.load_word2vec_format(pubmed_w2v,binary=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1MRruLUzSIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e86e79c-b290-43d1-e4d6-5d95cdcd755f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \"\"\"\n"
          ]
        }
      ],
      "source": [
        "#save all word embeddings to matrix\n",
        "vocab = np.zeros((len(model.wv.vocab)+1,200))\n",
        "word2idx = {}\n",
        "\n",
        "for key,val in model.wv.vocab.items():\n",
        "    idx = val.__dict__['index'] + 1\n",
        "    vocab[idx,:] = model[key]\n",
        "    word2idx[key] = idx\n",
        "\n",
        "#add additional word embedding for unknown words\n",
        "unk = len(vocab)\n",
        "vocab = np.vstack((vocab, np.zeros((1,200))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize embeddings\n",
        "vocab -= vocab.mean()\n",
        "vocab /= (vocab.std()*2.5)\n",
        "vocab[0,:] = 0"
      ],
      "metadata": {
        "id": "-fp7x9jdz8nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert words to indices\n",
        "text_idx = np.zeros((len(sentences),max_len))\n",
        "for i,sent in enumerate(sentences):\n",
        "    idx = [word2idx[word] if word in word2idx else unk for word in sent][:max_len]\n",
        "    l = len(idx)\n",
        "    text_idx[i,:l] = idx\n"
      ],
      "metadata": {
        "id": "SuHsjcSs5xVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save data\n",
        "with open('/content/drive/MyDrive/Pubmed/word2idx.pkl','wb') as f:\n",
        "    pickle.dump(word2idx,f)\n",
        "\n",
        "np.save('/content/drive/MyDrive/Pubmed/vocab.npy',vocab)\n",
        "np.save('/content/drive/MyDrive/Pubmed/X_medmentions.npy',text_idx)\n",
        "np.save('/content/drive/MyDrive/Pubmed/y_medmentions.npy',labels_notype)"
      ],
      "metadata": {
        "id": "ZSIJ2Xar5xfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Pubmed/sentence_lens_medmentions.pkl','wb') as f:\n",
        "    pickle.dump(sentence_lens,f)"
      ],
      "metadata": {
        "id": "Hz-gPhiEF4VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing for NER Corpora"
      ],
      "metadata": {
        "id": "tPOJuf5I7lvT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import csv"
      ],
      "metadata": {
        "id": "UBX_qSzV5xnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/NERdata'"
      ],
      "metadata": {
        "id": "p-u1fTX17BOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# csv dialect\n",
        "class TSV(csv.Dialect):\n",
        "    delimiter = '\\t'\n",
        "    doublequote = False\n",
        "    lineterminator = '\\n'\n",
        "    quoting = csv.QUOTE_NONE\n",
        "    strict = True"
      ],
      "metadata": {
        "id": "ezX8lIvl6QvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasets to test on\n",
        "datasets = [\n",
        "            'BC2GM',\n",
        "            'BC4CHEMD', \n",
        "            'NCBI-disease',\n",
        "            's800'\n",
        "           ]"
      ],
      "metadata": {
        "id": "mJS3tTjx6Q4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dictionaries\n",
        "with open('/content/drive/MyDrive/Pubmed/word2idx.pkl','rb') as f:\n",
        "    word2idx = pickle.load(f)\n",
        "unk = len(word2idx)\n",
        "label2idx = {'O':0, 'B':1, 'I': 2}"
      ],
      "metadata": {
        "id": "Y44HlWmv6Q6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save data\n",
        "for dataset in datasets:\n",
        "    \n",
        "    print('preparing',dataset)\n",
        "    path = os.path.join(data_path,dataset)\n",
        "\n",
        "    for split in ['train','train_dev','test']:\n",
        "\n",
        "        # read in tsv\n",
        "        reader = csv.reader(open(os.path.join(path,'%s.tsv' % split)), dialect=TSV)\n",
        "        \n",
        "        all_sentences = []\n",
        "        all_labels = []\n",
        "        \n",
        "        current_sentence = []\n",
        "        current_labels = []\n",
        "        \n",
        "        for i,row in enumerate(reader):\n",
        "        \n",
        "            if len(row) == 0:\n",
        "                all_sentences.append(current_sentence)\n",
        "                all_labels.append(current_labels)\n",
        "                current_sentence = []\n",
        "                current_labels = []\n",
        "            else:\n",
        "                word = row[0]\n",
        "                word = word2idx[word] if word in word2idx else unk\n",
        "                label = row[1]\n",
        "                label = label2idx[label]\n",
        "                current_sentence.append(word)\n",
        "                current_labels.append(label)\n",
        "\n",
        "        # pad everything to the correct max length\n",
        "        max_len = 50\n",
        "        doc_lens = [len(s[:max_len]) for s in all_sentences]\n",
        "        \n",
        "        X = np.zeros((len(all_sentences),max_len)).astype(np.int32)\n",
        "        y = np.zeros((len(all_labels),max_len)).astype(np.int32)\n",
        "        \n",
        "        for i,(l,s) in enumerate(zip(doc_lens,all_sentences)):\n",
        "            X[i,:l] = s[:max_len]\n",
        "        for i,(l,s) in enumerate(zip(doc_lens,all_labels)):\n",
        "            y[i,:l] = s[:max_len]\n",
        "        \n",
        "        np.save(\"/content/drive/MyDrive/Pubmed/%s_X_%s.npy\" % (dataset,split),X)\n",
        "        np.save(\"/content/drive/MyDrive/Pubmed/%s_y_%s.npy\" % (dataset,split),y)\n",
        "        with open('/content/drive/MyDrive/Pubmed/%s_senlens_%s.pkl' % (dataset,split),'wb') as f:\n",
        "            pickle.dump(doc_lens,f)\n"
      ],
      "metadata": {
        "id": "cyWCJNVg6Q9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcabd4fe-4d6f-4f81-f7a2-196f70dccb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preparing BC2GM\n",
            "preparing BC4CHEMD\n",
            "preparing BC5CDR-chem\n",
            "preparing BC5CDR-disease\n",
            "preparing JNLPBA\n",
            "preparing NCBI-disease\n",
            "preparing linnaeus\n",
            "preparing s800\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Data Preprocessing",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}