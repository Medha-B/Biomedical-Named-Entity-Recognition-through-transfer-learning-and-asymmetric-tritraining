{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook for writing the base BiLSTM-CRF model. \n",
        "\n",
        "Reference: Gao S, Kotevska O, Sorokine A, Christian JB (2021) A pre-training and self-training approach for biomedical named entity recognition. PLoS ONE 16(2): e0246310. https://doi.org/10.1371/journal.pone.0246310\n",
        "\n",
        "Code: https://code.ornl.gov/biomedner/biomedner"
      ],
      "metadata": {
        "id": "QoOOAjSsfxAW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJnCHtHMOXJL"
      },
      "source": [
        "Mounting the Google Drive and setting appropriate tensorflow version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMM3-x9vMpQZ",
        "outputId": "75a6f9dd-8ca2-4a13-dea4-57dab2f37959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IXQGAZFM2fo",
        "outputId": "96b2ac33-a80b-44f1-9cb7-be228f629bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "parzaBmMM2hX",
        "outputId": "a6854562-f3b2-4423-8b67-45203c158355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fur7BC_hQ1U1"
      },
      "source": [
        "Pre-training BiLSTM-CRF on Medmentions data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ51_MYjM2kD",
        "outputId": "ac926a95-254f-4bab-d640-441e514ada8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nervaluate\n",
            "  Downloading nervaluate-0.1.8-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: nervaluate\n",
            "Successfully installed nervaluate-0.1.8\n"
          ]
        }
      ],
      "source": [
        "pip install nervaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlC6tg--M2mG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.crf import crf_log_likelihood, crf_decode\n",
        "from tensorflow.contrib.rnn import LSTMCell, GRUCell, BasicRNNCell\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from nervaluate import Evaluator\n",
        "import sys\n",
        "import random\n",
        "import time\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HgH3gk9N_j6"
      },
      "outputs": [],
      "source": [
        "class lstm_crf(object):\n",
        "\n",
        "    '''\n",
        "    lstm-crf for ner tagging\n",
        "    \n",
        "    parameters:\n",
        "      - embedding_matrix: numpy array[float]\n",
        "        numpy array of word embeddings\n",
        "        each row should represent a word embedding\n",
        "        NOTE: the word index 0 is dropped, so the first row is ignored\n",
        "      - num_classes: int\n",
        "        number of output classes\n",
        "      - max_len: int (default: 50)\n",
        "        maximum number of input tokens in any sequence\n",
        "      - rnn_size: int (default: 300)\n",
        "        number of rnn units in RNN layer\n",
        "      - dropout_keep: float (default: 0.9)\n",
        "        dropout keep rate after rnn layer\n",
        "      - lr: float (default: 1E-4)\n",
        "        learning rate for adam optimizer\n",
        "       \n",
        "    methods:\n",
        "      - train(X,y,doc_lens,batch_size=128,epochs=25,patience=10,\n",
        "              validation_data=None,savebest=False,filepath=None)\n",
        "        train network on given data\n",
        "      - predict(X,doc_lens,batch_size=128)\n",
        "        return the predicted labels, flattened labels ignoring padding tokens, \n",
        "        and confidence scores for given data\n",
        "      - score(X,y,doc_lens,batch_size=128)\n",
        "        return the entity-level exact F1 score for given input sequences\n",
        "      - save(filepath)\n",
        "        save the model weights to a file\n",
        "      - load(filepath)\n",
        "        load model weights from a file\n",
        "    '''\n",
        "\n",
        "    def __init__(self,embedding_matrix,num_classes,max_len=50,rnn_size=300,dropout_keep=0.9,learning_rate=1E-4):\n",
        "    \n",
        "        #model params\n",
        "        self.num_classes = num_classes\n",
        "        self.embeddings = embedding_matrix.astype(np.float32)\n",
        "        self.max_len = max_len\n",
        "        self.rnn_size = rnn_size\n",
        "        self.dropout_keep = dropout_keep\n",
        "        self.idx2label = {0:'O',1:'B-ENT',2:'I-ENT'}\n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        #model inputs\n",
        "        self.doc_inputs = tf.placeholder(tf.float32,shape=[None,max_len,200])\n",
        "        self.doc_lens = tf.placeholder(tf.int32,shape=[None])\n",
        "        self.labels = tf.placeholder(tf.int32,shape=[None,max_len])\n",
        "        self.doc_idx = tf.placeholder(tf.int32,shape=[None,2])\n",
        "        self.dropout = tf.placeholder(tf.float32)\n",
        "        rnn_input = tf.nn.dropout(self.doc_inputs,self.dropout)\n",
        "                      \n",
        "        #bi-lstm\n",
        "        with tf.variable_scope('rnn',initializer=tf.contrib.layers.xavier_initializer()):\n",
        "            [outputs_fw,outputs_bw],_ = tf.nn.bidirectional_dynamic_rnn(\n",
        "                            LSTMCell(self.rnn_size/2),LSTMCell(self.rnn_size/2),\n",
        "                            rnn_input,sequence_length=self.doc_lens,dtype=tf.float32)\n",
        "            outputs = tf.concat((outputs_fw,outputs_bw),2)\n",
        "            outputs = tf.nn.dropout(outputs,self.dropout)\n",
        "        \n",
        "        #linear chain conditional random field\n",
        "        unary_scores = tf.layers.dense(outputs,self.num_classes,\n",
        "                       kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
        "        log_likelihood, self.transition_params = \\\n",
        "                       crf_log_likelihood(unary_scores,self.labels,self.doc_lens)\n",
        "        self.pred, viterbi_score = crf_decode(unary_scores,self.transition_params,self.doc_lens)\n",
        "        self.pred_flat = tf.gather_nd(self.pred,self.doc_idx)\n",
        "        seq_score,_ = crf_log_likelihood(unary_scores,self.pred,self.doc_lens,\n",
        "                                         self.transition_params)\n",
        "        self.seq_score = seq_score/tf.cast(self.doc_lens,tf.float32)\n",
        "\n",
        "        #loss, accuracy, and training functions\n",
        "        self.loss = tf.reduce_mean(-log_likelihood)\n",
        "        self.optimizer = tf.train.AdamOptimizer(self.lr,0.9,0.99).minimize(self.loss)\n",
        "        \n",
        "        #init ops\n",
        "        config = tf.ConfigProto()\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.sess = tf.Session(config=config)\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    def _gen_doc_idx(self,doc_lens):\n",
        "    \n",
        "        doc_idx = []\n",
        "        for i,l in enumerate(doc_lens):\n",
        "            for j in range(l):\n",
        "                doc_idx.append([i,j])\n",
        "        doc_idx = np.array(doc_idx)\n",
        "\n",
        "        return doc_idx\n",
        "        \n",
        "    def _flatten_y(self,y,doc_lens):\n",
        "    \n",
        "        y_flattened = []\n",
        "        for i,(doc,l) in enumerate(zip(y,doc_lens)):       \n",
        "            y_flattened.extend(doc[:l])\n",
        "            \n",
        "        return y_flattened\n",
        "                    \n",
        "    def train(self,X,y,doc_lens,batch_size=128,epochs=500,patience=10,\n",
        "              validation_data=None,savebest=False,filepath=None):\n",
        "    \n",
        "        '''\n",
        "        train network on given data\n",
        "        \n",
        "        parameters:\n",
        "          - X: numpy array[int]\n",
        "            2d numpy array (doc x word ids) of input data\n",
        "          - y: numpy array[int]\n",
        "            2d numpy array (doc x ner labels) of labels for given data\n",
        "          - doc_lens: list[int]\n",
        "            true sequence length of each sample\n",
        "          - batch size: int (default: 128)\n",
        "            batch size to use for training\n",
        "          - epochs: int (default: 500)\n",
        "            number of epochs to train for\n",
        "          - patience: int (default: 10)\n",
        "            training stops after no improvement in validation score\n",
        "            for this number of epochs\n",
        "          - validation_data: tuple (optional)\n",
        "            tuple of numpy arrays (X,y) representing validation data\n",
        "          - savebest: boolean (default: False)\n",
        "            set to True to save the best model based on validation score per epoch\n",
        "          - filepath: string (optional)\n",
        "            path to save model if savebest is set to True\n",
        "        \n",
        "        outputs:\n",
        "            None\n",
        "        '''\n",
        "    \n",
        "        if savebest==True and filepath==None:\n",
        "            raise Exception(\"Please enter a path to save the network\")\n",
        "\n",
        "        if validation_data:\n",
        "            validation_size = len(validation_data[0])\n",
        "        else:\n",
        "            validation_size = len(X)\n",
        "\n",
        "        print('training network on %i documents, validation on %i documents' \\\n",
        "              % (len(X), validation_size))\n",
        "\n",
        "        #track best model for saving\n",
        "        prevbest = 0\n",
        "        pat_count = 0\n",
        "\n",
        "        for ep in range(epochs):\n",
        "\n",
        "            #shuffle data\n",
        "            xyz = list(zip(X,y,doc_lens))            \n",
        "            random.shuffle(xyz)\n",
        "            X,y,doc_lens = zip(*xyz)\n",
        "            X = list(X)\n",
        "            y = list(y)\n",
        "            doc_lens = list(doc_lens)\n",
        "\n",
        "            y_pred = []\n",
        "            y_true = []\n",
        "            start_time = time.time()\n",
        "\n",
        "            #train\n",
        "            for start in range(0,len(X),batch_size):\n",
        "\n",
        "                #get batch index\n",
        "                if start+batch_size < len(X):\n",
        "                    stop = start+batch_size\n",
        "                else:\n",
        "                    stop = len(X)\n",
        "\n",
        "                embeds = np.take(self.embeddings,X[start:stop],0)\n",
        "                feed_dict = {self.doc_inputs:embeds,\n",
        "                             self.labels:y[start:stop],\n",
        "                             self.doc_lens:doc_lens[start:stop],\n",
        "                             self.doc_idx:self._gen_doc_idx(doc_lens[start:stop]),\n",
        "                             self.dropout:self.dropout_keep}\n",
        "                preds,loss,_ = self.sess.run([self.pred,self.loss,self.optimizer],\n",
        "                              feed_dict=feed_dict)\n",
        "\n",
        "                #track correct predictions\n",
        "                for y_pred_,y_true_,l in zip(preds,y[start:stop],doc_lens[start:stop]):\n",
        "                    y_p = [self.idx2label[l] for l in y_pred_[:l]]\n",
        "                    y_t = [self.idx2label[l] for l in y_true_[:l]]\n",
        "                    y_pred.append(y_p)\n",
        "                    y_true.append(y_t)\n",
        "                    \n",
        "                sys.stdout.write(\"epoch %i, sample %i of %i, loss: %f        \\r\"\\\n",
        "                                 % (ep+1,stop,len(X),loss))\n",
        "                sys.stdout.flush()\n",
        "\n",
        "            #checkpoint after every epoch\n",
        "            print(\"\\ntraining time: %.2f\" % (time.time()-start_time))\n",
        "            evaluator = Evaluator(y_true, y_pred, tags=['ENT'], loader=\"list\")\n",
        "            results, results_by_tag = evaluator.evaluate()\n",
        "            f1 = results['exact']['f1']\n",
        "            print(\"epoch %i training f1: %.4f\" % (ep+1,f1))\n",
        "\n",
        "            f1 = self.score(validation_data[0],validation_data[1],\n",
        "                            validation_data[2],batch_size=batch_size)\n",
        "            print(\"epoch %i validation f1: %.4f\" % (ep+1,f1))\n",
        "\n",
        "            #save if performance better than previous best\n",
        "            if f1 >= prevbest:\n",
        "                prevbest = f1\n",
        "                pat_count = 0\n",
        "                if savebest:\n",
        "                    self.save(filepath)\n",
        "            else:\n",
        "                pat_count += 1\n",
        "                if pat_count >= patience:\n",
        "                    break\n",
        "\n",
        "            #reset timer\n",
        "            start_time = time.time()\n",
        "\n",
        "    def predict(self,X,doc_lens,batch_size=128):\n",
        "    \n",
        "        '''\n",
        "        return the predicted labels, flattened labels ignoring padding tokens, \n",
        "        and confidence scores for given data\n",
        "        \n",
        "        parameters:\n",
        "          - X: numpy array[int]\n",
        "            2d numpy array (doc x word ids) of input data\n",
        "          - doc_lens: list[int]\n",
        "            true sequence length of each sample\n",
        "          - batch size: int (default: 128)\n",
        "            batch size to use for inference\n",
        "            \n",
        "        outputs:\n",
        "          - y_pred: numpy_array[int]\n",
        "            2d numpy array of predicted labels for input data\n",
        "          - y_pred_flat: list[int]\n",
        "            flattened list of all predicted labels ignoring padding tokens\n",
        "          - scores: numpy_array[float]\n",
        "            flattened list of confidence scores for all predicted labels ignoring padding tokens\n",
        "        '''\n",
        "    \n",
        "        y_pred_flat = []\n",
        "        y_pred = []\n",
        "        scores = []\n",
        "        \n",
        "        for start in range(0,len(X),batch_size):\n",
        "\n",
        "            #get batch index\n",
        "            if start+batch_size < len(X):\n",
        "                stop = start+batch_size\n",
        "            else:\n",
        "                stop = len(X)\n",
        "\n",
        "            embeds = np.take(self.embeddings,X[start:stop],0)\n",
        "            feed_dict = {self.doc_inputs:embeds,\n",
        "                         self.doc_lens:doc_lens[start:stop],\n",
        "                         self.doc_idx:self._gen_doc_idx(doc_lens[start:stop]),\n",
        "                         self.dropout:1.0}\n",
        "            pred,pred_flat,score = self.sess.run([self.pred,self.pred_flat,self.seq_score],\n",
        "                                   feed_dict=feed_dict)\n",
        "\n",
        "            score = np.exp(score)\n",
        "            y_pred.append(pred)\n",
        "            y_pred_flat.extend(pred_flat)\n",
        "            scores.extend(score)\n",
        "\n",
        "            sys.stdout.write(\"processed %i of %i records        \\r\" \\\n",
        "                             % (stop,len(X)))\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        print()\n",
        "        y_pred = np.vstack(y_pred)\n",
        "        return y_pred,y_pred_flat,np.array(scores)\n",
        "\n",
        "    def score(self,X,y,doc_lens,batch_size=128):\n",
        "    \n",
        "        '''\n",
        "        return the entity-level exact F1 score for given input sequences\n",
        "        \n",
        "        parameters:\n",
        "          - X: numpy array[int]\n",
        "            2d numpy array (doc x word ids) of input data\n",
        "          - y: numpy array[int]\n",
        "            2d numpy array (doc x ner labels) of labels for given data\n",
        "          - doc_lens: list[int]\n",
        "            true sequence length of each sample\n",
        "          - batch size: int (default: 128)\n",
        "            batch size to use for inference\n",
        "            \n",
        "        outputs:\n",
        "          - entity-level exact F1 score for given input sequences\n",
        "        '''\n",
        "        \n",
        "        y_preds_,_,_ = self.predict(X,doc_lens,batch_size)\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        for y_pred_,y_true_,l in zip(y_preds_,y,doc_lens):\n",
        "            y_p = [self.idx2label[l] for l in y_pred_[:l]]\n",
        "            y_t = [self.idx2label[l] for l in y_true_[:l]]\n",
        "            y_pred.append(y_p)\n",
        "            y_true.append(y_t)\n",
        "        \n",
        "        evaluator = Evaluator(y_true, y_pred, tags=['ENT'], loader=\"list\")\n",
        "        results, results_by_tag = evaluator.evaluate()\n",
        "        f1 = results['exact']['f1']\n",
        "        \n",
        "        return f1\n",
        "\n",
        "    def save(self,filename):\n",
        "    \n",
        "        '''\n",
        "        save the model weights to a file\n",
        "        \n",
        "        parameters:\n",
        "          - filepath: string\n",
        "            path to save model weights\n",
        "        \n",
        "        outputs:\n",
        "            None\n",
        "        '''\n",
        "        \n",
        "        self.saver.save(self.sess,filename)\n",
        "\n",
        "    def load(self,filename):\n",
        "    \n",
        "        '''\n",
        "        load model weights from a file\n",
        "        \n",
        "        parameters:\n",
        "          - filepath: string\n",
        "            path from which to load model weights\n",
        "        \n",
        "        outputs:\n",
        "            None\n",
        "        '''\n",
        "        \n",
        "        self.saver.restore(self.sess,filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IA0p-N-RM2oi"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "patience = 5\n",
        "pretrain_dataset = 'medmentions'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJaKFjU_5HxL"
      },
      "outputs": [],
      "source": [
        "vocab = np.load('/content/drive/MyDrive/Pubmed/vocab.npy').astype(np.int16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-3q_Wh_M2qf"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "vocab = np.load('/content/drive/MyDrive/Pubmed/vocab.npy')\n",
        "X = np.load('/content/drive/MyDrive/Pubmed/X_%s.npy' % pretrain_dataset).astype(np.int32)\n",
        "y = np.load('/content/drive/MyDrive/Pubmed/y_%s.npy' % pretrain_dataset).astype(np.int32)\n",
        "max_len = X.shape[1]\n",
        "num_classes = np.max(y) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auO7ZoM3M2tB"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/Pubmed/sentence_lens_%s.pkl' % pretrain_dataset,'rb') as f:\n",
        "    doc_len = pickle.load(f)\n",
        "doc_len = [l if l <=max_len else max_len for l in doc_len]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnhTuVNNM2ub"
      },
      "outputs": [],
      "source": [
        "# load conversion dictionaries\n",
        "label2idx = {'O':0, 'B':1, 'I': 2}\n",
        "idx2label = {v:k for k,v in label2idx.items()}\n",
        "with open('/content/drive/MyDrive/Pubmed/word2idx.pkl','rb') as f:\n",
        "    word2idx = pickle.load(f)\n",
        "idx2word = {v:k for k,v in word2idx.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13z75Y5oM2xc"
      },
      "outputs": [],
      "source": [
        "# train val split\n",
        "num_docs = len(X)\n",
        "train_size = int(num_docs * 0.8)\n",
        "X_train = X[:train_size]\n",
        "X_val = X[train_size:]\n",
        "y_train = y[:train_size]\n",
        "y_val = y[train_size:]\n",
        "doc_len_train = doc_len[:train_size]\n",
        "doc_len_val = doc_len[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVLosiBM2z0",
        "outputId": "bfed108d-32ee-48ca-ba0a-4699f3d74793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-38d7e2192170>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From <ipython-input-6-38d7e2192170>:54: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-38d7e2192170>:59: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From <ipython-input-6-38d7e2192170>:59: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-38d7e2192170>:60: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From <ipython-input-6-38d7e2192170>:60: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:47 tensorflow WARNING: From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-38d7e2192170>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:48 tensorflow WARNING: From <ipython-input-6-38d7e2192170>:66: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 09:33:48 tensorflow WARNING: From /tensorflow-1.15.2/python3.7/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training network on 38184 documents, validation on 9546 documents\n",
            "\n",
            "training time: 133.28\n",
            "epoch 1 training f1: 0.3511\n",
            "\n",
            "epoch 1 validation f1: 0.5259\n",
            "\n",
            "training time: 130.68\n",
            "epoch 2 training f1: 0.5476\n",
            "\n",
            "epoch 2 validation f1: 0.5762\n",
            "\n",
            "training time: 128.85\n",
            "epoch 3 training f1: 0.5748\n",
            "\n",
            "epoch 3 validation f1: 0.5909\n",
            "\n",
            "training time: 131.37\n",
            "epoch 4 training f1: 0.5915\n",
            "\n",
            "epoch 4 validation f1: 0.6062\n",
            "\n",
            "training time: 130.77\n",
            "epoch 5 training f1: 0.6042\n",
            "\n",
            "epoch 5 validation f1: 0.6148\n",
            "\n",
            "training time: 130.23\n",
            "epoch 6 training f1: 0.6141\n",
            "\n",
            "epoch 6 validation f1: 0.6251\n",
            "\n",
            "training time: 129.91\n",
            "epoch 7 training f1: 0.6216\n",
            "\n",
            "epoch 7 validation f1: 0.6291\n",
            "\n",
            "training time: 130.31\n",
            "epoch 8 training f1: 0.6299\n",
            "\n",
            "epoch 8 validation f1: 0.6333\n",
            "\n",
            "training time: 129.44\n",
            "epoch 9 training f1: 0.6349\n",
            "\n",
            "epoch 9 validation f1: 0.6383\n",
            "\n",
            "training time: 129.61\n",
            "epoch 10 training f1: 0.6410\n",
            "\n",
            "epoch 10 validation f1: 0.6420\n",
            "\n",
            "training time: 128.54\n",
            "epoch 11 training f1: 0.6461\n",
            "\n",
            "epoch 11 validation f1: 0.6436\n",
            "\n",
            "training time: 130.29\n",
            "epoch 12 training f1: 0.6510\n",
            "\n",
            "epoch 12 validation f1: 0.6472\n",
            "\n",
            "training time: 128.07\n",
            "epoch 13 training f1: 0.6542\n",
            "\n",
            "epoch 13 validation f1: 0.6491\n",
            "\n",
            "training time: 129.07\n",
            "epoch 14 training f1: 0.6581\n",
            "\n",
            "epoch 14 validation f1: 0.6539\n",
            "\n",
            "training time: 129.00\n",
            "epoch 15 training f1: 0.6621\n",
            "\n",
            "epoch 15 validation f1: 0.6542\n",
            "\n",
            "training time: 129.27\n",
            "epoch 16 training f1: 0.6649\n",
            "\n",
            "epoch 16 validation f1: 0.6574\n",
            "\n",
            "training time: 128.74\n",
            "epoch 17 training f1: 0.6674\n",
            "\n",
            "epoch 17 validation f1: 0.6581\n",
            "\n",
            "training time: 126.80\n",
            "epoch 18 training f1: 0.6710\n",
            "\n",
            "epoch 18 validation f1: 0.6604\n",
            "\n",
            "training time: 130.04\n",
            "epoch 19 training f1: 0.6732\n",
            "\n",
            "epoch 19 validation f1: 0.6615\n",
            "\n",
            "training time: 128.31\n",
            "epoch 20 training f1: 0.6761\n",
            "\n",
            "epoch 20 validation f1: 0.6620\n",
            "\n",
            "training time: 131.37\n",
            "epoch 21 training f1: 0.6795\n",
            "\n",
            "epoch 21 validation f1: 0.6648\n",
            "\n",
            "training time: 129.70\n",
            "epoch 22 training f1: 0.6813\n",
            "\n",
            "epoch 22 validation f1: 0.6648\n",
            "\n",
            "training time: 129.95\n",
            "epoch 23 training f1: 0.6829\n",
            "\n",
            "epoch 23 validation f1: 0.6658\n",
            "\n",
            "training time: 128.93\n",
            "epoch 24 training f1: 0.6870\n",
            "\n",
            "epoch 24 validation f1: 0.6586\n",
            "\n",
            "training time: 130.16\n",
            "epoch 25 training f1: 0.6882\n",
            "\n",
            "epoch 25 validation f1: 0.6634\n",
            "\n",
            "training time: 128.06\n",
            "epoch 26 training f1: 0.6898\n",
            "\n",
            "epoch 26 validation f1: 0.6681\n",
            "\n",
            "training time: 131.63\n",
            "epoch 27 training f1: 0.6913\n",
            "\n",
            "epoch 27 validation f1: 0.6665\n",
            "\n",
            "training time: 128.99\n",
            "epoch 28 training f1: 0.6941\n",
            "\n",
            "epoch 28 validation f1: 0.6689\n",
            "\n",
            "training time: 129.17\n",
            "epoch 29 training f1: 0.6953\n",
            "\n",
            "epoch 29 validation f1: 0.6688\n",
            "\n",
            "training time: 127.75\n",
            "epoch 30 training f1: 0.6975\n",
            "\n",
            "epoch 30 validation f1: 0.6692\n",
            "\n",
            "training time: 129.15\n",
            "epoch 31 training f1: 0.6989\n",
            "\n",
            "epoch 31 validation f1: 0.6700\n",
            "\n",
            "training time: 127.90\n",
            "epoch 32 training f1: 0.7020\n",
            "\n",
            "epoch 32 validation f1: 0.6710\n",
            "\n",
            "training time: 129.21\n",
            "epoch 33 training f1: 0.7028\n",
            "\n",
            "epoch 33 validation f1: 0.6697\n",
            "\n",
            "training time: 128.99\n",
            "epoch 34 training f1: 0.7048\n",
            "\n",
            "epoch 34 validation f1: 0.6702\n",
            "\n",
            "training time: 129.27\n",
            "epoch 35 training f1: 0.7063\n",
            "\n",
            "epoch 35 validation f1: 0.6719\n",
            "\n",
            "training time: 128.20\n",
            "epoch 36 training f1: 0.7090\n",
            "\n",
            "epoch 36 validation f1: 0.6705\n",
            "\n",
            "training time: 126.69\n",
            "epoch 37 training f1: 0.7091\n",
            "\n",
            "epoch 37 validation f1: 0.6723\n",
            "\n",
            "training time: 127.75\n",
            "epoch 38 training f1: 0.7118\n",
            "\n",
            "epoch 38 validation f1: 0.6689\n",
            "\n",
            "training time: 128.08\n",
            "epoch 39 training f1: 0.7149\n",
            "\n",
            "epoch 39 validation f1: 0.6738\n",
            "\n",
            "training time: 126.69\n",
            "epoch 40 training f1: 0.7146\n",
            "\n",
            "epoch 40 validation f1: 0.6712\n",
            "\n",
            "training time: 127.94\n",
            "epoch 41 training f1: 0.7163\n",
            "\n",
            "epoch 41 validation f1: 0.6708\n",
            "\n",
            "training time: 126.63\n",
            "epoch 42 training f1: 0.7181\n",
            "\n",
            "epoch 42 validation f1: 0.6683\n",
            "\n",
            "training time: 125.35\n",
            "epoch 43 training f1: 0.7195\n",
            "\n",
            "epoch 43 validation f1: 0.6718\n",
            "\n",
            "training time: 128.54\n",
            "epoch 44 training f1: 0.7207\n",
            "\n",
            "epoch 44 validation f1: 0.6711\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "model = lstm_crf(vocab,num_classes,max_len)\n",
        "model.train(X_train,y_train,doc_len_train,\n",
        "            batch_size=batch_size,patience=patience,\n",
        "            validation_data=(X_val,y_val,doc_len_val),\n",
        "            savebest=True,filepath='/content/drive/MyDrive/Pubmed/PretrainModel/lstm_crf_%s.ckpt' % pretrain_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRHL6BTCM22B",
        "outputId": "09f7a5c4-db30-4341-a86c-2fff3522c477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Pubmed/PretrainModel/lstm_crf_medmentions.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-20 11:31:42 tensorflow INFO: Restoring parameters from /content/drive/MyDrive/Pubmed/PretrainModel/lstm_crf_medmentions.ckpt\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "processed 1 of 1 records        \r\n",
            "sentence: ['To', 'identify', 'key', 'genes', 'and', 'microRNA', 's', 'in', 'MM', ',', 'we', 'downloaded', 'two', 'gene', 'expression', 'profiles', '(', 'UNK', 'and', 'UNK', ')', 'and', 'two', 'microRNA', 'expression', 'profiles', '(', 'UNK', 'and', 'UNK', ')', 'from', 'the', 'Gene', 'Expression', 'Omnibus', '(', 'GEO', ')', 'database', '.']\n",
            "true labels: ['O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'O']\n",
            "score: [0.906443]\n",
            "\n",
            "\n",
            "sentence: ['A', 'total', 'of', '596', 'differentially', 'expressed', 'genes', '(', 'DEGs', ')', 'and', '39', 'differentially', 'expressed', 'microRNAs', '(', 'DEMs', ')', 'were', 'screened', 'out', '.']\n",
            "true labels: ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O']\n",
            "score: [0.8767411]\n",
            "\n",
            "\n",
            "sentence: ['Pathway', 'analysis', 'showed', 'that', 'upregulated', 'genes', 'were', 'mainly', 'enriched', 'in', 'the', '\"', 'B', 'cell', 'receptor', 'signaling', 'pathway', '\"', ',', '\"', 'Cell', 'cycle', '\"', 'and', '\"', 'NF', '-', 'kappa', 'B', 'signaling', 'pathway', '\"', ',', 'whereas', 'downregulated', 'genes', 'were', 'mainly', 'enriched', 'in', 'the', '\"', 'Ribosome', '\"', ',', '\"', 'FoxO', 'signaling', 'pathway', '\"', 'and', '\"', 'p53', 'signaling', 'pathway', '\"', '.']\n",
            "true labels: ['B', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O']\n",
            "pred labels: ['B', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O']\n",
            "score: [0.9417938]\n",
            "\n",
            "\n",
            "sentence: ['We', 'subsequently', 'constructed', 'a', 'protein', '-', 'protein', 'interaction', 'network', 'of', 'DEGs', 'consisting', 'of', '277', 'genes', 'and', '563', 'interactions', '.']\n",
            "true labels: ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'O']\n",
            "score: [0.9443611]\n",
            "\n",
            "\n",
            "sentence: ['In', 'addition', ',', '32', 'genes', 'with', 'high', 'degrees', 'in', 'the', 'network', 'were', 'identified', 'as', 'hub', 'genes', 'in', 'MM', ',', 'e', '.']\n",
            "true labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O']\n",
            "score: [0.90474415]\n",
            "\n",
            "\n",
            "sentence: ['g', '.']\n",
            "true labels: ['O', 'O']\n",
            "pred labels: ['O', 'O']\n",
            "score: [0.9473215]\n",
            "\n",
            "\n",
            "sentence: ['HDAC2', ',', 'RBBP4', ',', 'CREB1', ',', 'and', 'RB1', '.']\n",
            "true labels: ['B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O']\n",
            "pred labels: ['B', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O']\n",
            "score: [0.9937937]\n",
            "\n",
            "\n",
            "sentence: ['Additionally', ',', 'we', 'constructed', 'a', 'microRNA', '-', 'mRNA', 'regulatory', 'network', 'depicting', 'interactions', 'between', 'DEMs', 'and', 'their', 'targets', ',', 'including', 'the', 'miR', '-', 'UNK', '-', 'GADD45A', 'and', 'miR', '-', '148a', '-', 'USPL1', 'pairs', '.']\n",
            "true labels: ['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'B', 'I', 'I', 'O', 'B', 'O', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'B', 'O']\n",
            "score: [0.900114]\n",
            "\n",
            "\n",
            "sentence: ['In', 'conclusion', ',', 'the', 'results', 'of', 'this', 'data', 'mining', 'and', 'integration', 'help', 'reveal', 'the', 'molecular', 'basis', 'of', 'MM', 'pathogenesis', 'as', 'well', 'as', 'potential', 'biomarkers', 'and', 'therapeutic', 'targets', 'for', 'MM', 'diagnosis', 'and', 'treatment', '.']\n",
            "true labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'I', 'O', 'B', 'B', 'O', 'B', 'O']\n",
            "pred labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'B', 'O', 'B', 'O']\n",
            "score: [0.8668256]\n",
            "\n",
            "\n",
            "sentence: ['As', 'primary', 'agents', 'of', 'socialization', ',', 'families', 'and', 'schools', 'can', 'powerfully', 'shape', 'the', 'academic', 'adaptation', 'of', 'youth', '.']\n",
            "true labels: ['O', 'B', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'O']\n",
            "pred labels: ['O', 'B', 'I', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'O']\n",
            "score: [0.8815178]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# load best model and show some examples to check that model is learning\n",
        "model.load('/content/drive/MyDrive/Pubmed/PretrainModel/lstm_crf_%s.ckpt' % pretrain_dataset)\n",
        "for i in range(10):\n",
        "    sentence = X_val[i:i+1]\n",
        "    labels = y_val[i:i+1]\n",
        "    doc_len = doc_len_val[i:i+1]\n",
        "    _,preds,score = model.predict(sentence,doc_len)\n",
        "    print('sentence:',[idx2word[w] if w in idx2word else 'UNK' \\\n",
        "                       for w in sentence[0] if w != 0])\n",
        "    print('true labels:',[idx2label[l] for l in labels[0] if l != -1])\n",
        "    print('pred labels:',[idx2label[l] for l in preds])\n",
        "    print('score:', score)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPq-sZChQ-_B"
      },
      "source": [
        "Training BiLSTM-CRF on NER datasets without using semi-supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO8JSYMHM23_"
      },
      "outputs": [],
      "source": [
        "batch_size_f = 128\n",
        "patience_f = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1_X2hBM26T"
      },
      "outputs": [],
      "source": [
        "# datasets to test on\n",
        "datasets = [\n",
        "            'BC2GM',\n",
        "            'BC4CHEMD', \n",
        "            'NCBI-disease',\n",
        "            's800'\n",
        "           ]\n",
        "\n",
        "# supervised dataset sizes to test on\n",
        "data_sizes = [\n",
        "              2000\n",
        "             ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WY86ik_6M274",
        "outputId": "93c997ab-e2b4-48ed-bac9-939b25a61aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training BC2GM 2000\n",
            "training network on 1600 documents, validation on 400 documents\n",
            "\n",
            "training time: 3.56\n",
            "epoch 1 training f1: 0.0325\n",
            "\n",
            "epoch 1 validation f1: 0.0324\n",
            "\n",
            "training time: 2.90\n",
            "epoch 2 training f1: 0.0320\n",
            "\n",
            "epoch 2 validation f1: 0.0305\n",
            "\n",
            "training time: 2.93\n",
            "epoch 3 training f1: 0.0311\n",
            "\n",
            "epoch 3 validation f1: 0.0266\n",
            "\n",
            "training time: 2.97\n",
            "epoch 4 training f1: 0.0272\n",
            "\n",
            "epoch 4 validation f1: 0.0161\n",
            "\n",
            "training time: 3.12\n",
            "epoch 5 training f1: 0.0157\n",
            "\n",
            "epoch 5 validation f1: 0.0089\n",
            "\n",
            "training time: 2.93\n",
            "epoch 6 training f1: 0.0079\n",
            "\n",
            "epoch 6 validation f1: 0.0109\n",
            "\n",
            "training time: 2.88\n",
            "epoch 7 training f1: 0.0136\n",
            "\n",
            "epoch 7 validation f1: 0.0108\n",
            "\n",
            "training time: 2.92\n",
            "epoch 8 training f1: 0.0214\n",
            "\n",
            "epoch 8 validation f1: 0.0218\n",
            "\n",
            "training time: 2.86\n",
            "epoch 9 training f1: 0.0316\n",
            "\n",
            "epoch 9 validation f1: 0.0255\n",
            "\n",
            "training time: 2.83\n",
            "epoch 10 training f1: 0.0386\n",
            "\n",
            "epoch 10 validation f1: 0.0290\n",
            "\n",
            "training time: 2.93\n",
            "epoch 11 training f1: 0.0549\n",
            "\n",
            "epoch 11 validation f1: 0.0429\n",
            "\n",
            "training time: 3.04\n",
            "epoch 12 training f1: 0.0640\n",
            "\n",
            "epoch 12 validation f1: 0.0623\n",
            "\n",
            "training time: 2.92\n",
            "epoch 13 training f1: 0.0711\n",
            "\n",
            "epoch 13 validation f1: 0.0624\n",
            "\n",
            "training time: 2.89\n",
            "epoch 14 training f1: 0.0884\n",
            "\n",
            "epoch 14 validation f1: 0.0681\n",
            "\n",
            "training time: 2.96\n",
            "epoch 15 training f1: 0.0886\n",
            "\n",
            "epoch 15 validation f1: 0.0771\n",
            "\n",
            "training time: 2.95\n",
            "epoch 16 training f1: 0.0980\n",
            "\n",
            "epoch 16 validation f1: 0.0812\n",
            "\n",
            "training time: 2.92\n",
            "epoch 17 training f1: 0.1005\n",
            "\n",
            "epoch 17 validation f1: 0.0851\n",
            "\n",
            "training time: 3.08\n",
            "epoch 18 training f1: 0.1043\n",
            "\n",
            "epoch 18 validation f1: 0.0982\n",
            "\n",
            "training time: 2.92\n",
            "epoch 19 training f1: 0.1154\n",
            "\n",
            "epoch 19 validation f1: 0.1027\n",
            "\n",
            "training time: 2.88\n",
            "epoch 20 training f1: 0.1231\n",
            "\n",
            "epoch 20 validation f1: 0.1048\n",
            "\n",
            "training time: 2.94\n",
            "epoch 21 training f1: 0.1236\n",
            "\n",
            "epoch 21 validation f1: 0.1154\n",
            "\n",
            "training time: 2.89\n",
            "epoch 22 training f1: 0.1252\n",
            "\n",
            "epoch 22 validation f1: 0.1226\n",
            "\n",
            "training time: 2.89\n",
            "epoch 23 training f1: 0.1342\n",
            "\n",
            "epoch 23 validation f1: 0.1241\n",
            "\n",
            "training time: 2.87\n",
            "epoch 24 training f1: 0.1319\n",
            "\n",
            "epoch 24 validation f1: 0.1264\n",
            "\n",
            "training time: 3.04\n",
            "epoch 25 training f1: 0.1329\n",
            "\n",
            "epoch 25 validation f1: 0.1322\n",
            "\n",
            "training time: 2.90\n",
            "epoch 26 training f1: 0.1409\n",
            "\n",
            "epoch 26 validation f1: 0.1357\n",
            "\n",
            "training time: 2.89\n",
            "epoch 27 training f1: 0.1496\n",
            "\n",
            "epoch 27 validation f1: 0.1424\n",
            "\n",
            "training time: 2.91\n",
            "epoch 28 training f1: 0.1425\n",
            "\n",
            "epoch 28 validation f1: 0.1414\n",
            "\n",
            "training time: 2.92\n",
            "epoch 29 training f1: 0.1535\n",
            "\n",
            "epoch 29 validation f1: 0.1441\n",
            "\n",
            "training time: 2.89\n",
            "epoch 30 training f1: 0.1592\n",
            "\n",
            "epoch 30 validation f1: 0.1402\n",
            "\n",
            "training time: 2.92\n",
            "epoch 31 training f1: 0.1569\n",
            "\n",
            "epoch 31 validation f1: 0.1446\n",
            "\n",
            "training time: 2.93\n",
            "epoch 32 training f1: 0.1586\n",
            "\n",
            "epoch 32 validation f1: 0.1473\n",
            "\n",
            "training time: 2.90\n",
            "epoch 33 training f1: 0.1588\n",
            "\n",
            "epoch 33 validation f1: 0.1511\n",
            "\n",
            "training time: 2.94\n",
            "epoch 34 training f1: 0.1657\n",
            "\n",
            "epoch 34 validation f1: 0.1547\n",
            "\n",
            "training time: 2.89\n",
            "epoch 35 training f1: 0.1751\n",
            "\n",
            "epoch 35 validation f1: 0.1540\n",
            "\n",
            "training time: 2.92\n",
            "epoch 36 training f1: 0.1724\n",
            "\n",
            "epoch 36 validation f1: 0.1561\n",
            "\n",
            "training time: 2.93\n",
            "epoch 37 training f1: 0.1703\n",
            "\n",
            "epoch 37 validation f1: 0.1556\n",
            "\n",
            "training time: 3.10\n",
            "epoch 38 training f1: 0.1744\n",
            "\n",
            "epoch 38 validation f1: 0.1619\n",
            "\n",
            "training time: 2.96\n",
            "epoch 39 training f1: 0.1804\n",
            "\n",
            "epoch 39 validation f1: 0.1607\n",
            "\n",
            "training time: 2.91\n",
            "epoch 40 training f1: 0.1784\n",
            "\n",
            "epoch 40 validation f1: 0.1612\n",
            "\n",
            "training time: 2.89\n",
            "epoch 41 training f1: 0.1819\n",
            "\n",
            "epoch 41 validation f1: 0.1686\n",
            "\n",
            "training time: 2.87\n",
            "epoch 42 training f1: 0.1908\n",
            "\n",
            "epoch 42 validation f1: 0.1813\n",
            "\n",
            "training time: 2.95\n",
            "epoch 43 training f1: 0.1920\n",
            "\n",
            "epoch 43 validation f1: 0.1784\n",
            "\n",
            "training time: 3.08\n",
            "epoch 44 training f1: 0.1925\n",
            "\n",
            "epoch 44 validation f1: 0.1851\n",
            "\n",
            "training time: 2.95\n",
            "epoch 45 training f1: 0.1861\n",
            "\n",
            "epoch 45 validation f1: 0.1847\n",
            "\n",
            "training time: 2.89\n",
            "epoch 46 training f1: 0.1936\n",
            "\n",
            "epoch 46 validation f1: 0.1856\n",
            "\n",
            "training time: 2.90\n",
            "epoch 47 training f1: 0.1964\n",
            "\n",
            "epoch 47 validation f1: 0.1886\n",
            "\n",
            "training time: 2.90\n",
            "epoch 48 training f1: 0.1989\n",
            "\n",
            "epoch 48 validation f1: 0.1957\n",
            "\n",
            "training time: 2.90\n",
            "epoch 49 training f1: 0.2005\n",
            "\n",
            "epoch 49 validation f1: 0.2025\n",
            "\n",
            "training time: 2.95\n",
            "epoch 50 training f1: 0.2077\n",
            "\n",
            "epoch 50 validation f1: 0.2053\n",
            "\n",
            "training time: 2.95\n",
            "epoch 51 training f1: 0.2091\n",
            "\n",
            "epoch 51 validation f1: 0.2040\n",
            "\n",
            "training time: 2.91\n",
            "epoch 52 training f1: 0.2089\n",
            "\n",
            "epoch 52 validation f1: 0.2049\n",
            "\n",
            "training time: 2.97\n",
            "epoch 53 training f1: 0.2181\n",
            "\n",
            "epoch 53 validation f1: 0.2239\n",
            "\n",
            "training time: 2.93\n",
            "epoch 54 training f1: 0.2097\n",
            "\n",
            "epoch 54 validation f1: 0.2045\n",
            "\n",
            "training time: 2.91\n",
            "epoch 55 training f1: 0.2134\n",
            "\n",
            "epoch 55 validation f1: 0.2182\n",
            "\n",
            "training time: 2.98\n",
            "epoch 56 training f1: 0.2228\n",
            "\n",
            "epoch 56 validation f1: 0.2382\n",
            "\n",
            "training time: 2.94\n",
            "epoch 57 training f1: 0.2236\n",
            "\n",
            "epoch 57 validation f1: 0.2334\n",
            "\n",
            "training time: 2.96\n",
            "epoch 58 training f1: 0.2257\n",
            "\n",
            "epoch 58 validation f1: 0.2193\n",
            "\n",
            "training time: 2.97\n",
            "epoch 59 training f1: 0.2244\n",
            "\n",
            "epoch 59 validation f1: 0.2242\n",
            "\n",
            "training time: 2.93\n",
            "epoch 60 training f1: 0.2334\n",
            "\n",
            "epoch 60 validation f1: 0.2266\n",
            "\n",
            "training time: 2.95\n",
            "epoch 61 training f1: 0.2270\n",
            "\n",
            "epoch 61 validation f1: 0.2327\n",
            "\n",
            "training time: 2.92\n",
            "epoch 62 training f1: 0.2273\n",
            "\n",
            "epoch 62 validation f1: 0.2247\n",
            "\n",
            "training time: 2.91\n",
            "epoch 63 training f1: 0.2320\n",
            "\n",
            "epoch 63 validation f1: 0.2342\n",
            "\n",
            "training time: 3.08\n",
            "epoch 64 training f1: 0.2369\n",
            "\n",
            "epoch 64 validation f1: 0.2321\n",
            "\n",
            "training time: 2.92\n",
            "epoch 65 training f1: 0.2361\n",
            "\n",
            "epoch 65 validation f1: 0.2313\n",
            "\n",
            "training time: 2.94\n",
            "epoch 66 training f1: 0.2363\n",
            "\n",
            "epoch 66 validation f1: 0.2373\n",
            "\n",
            "training time: 2.90\n",
            "epoch 67 training f1: 0.2395\n",
            "\n",
            "epoch 67 validation f1: 0.2335\n",
            "\n",
            "training time: 2.90\n",
            "epoch 68 training f1: 0.2505\n",
            "\n",
            "epoch 68 validation f1: 0.2405\n",
            "\n",
            "training time: 2.97\n",
            "epoch 69 training f1: 0.2531\n",
            "\n",
            "epoch 69 validation f1: 0.2412\n",
            "\n",
            "training time: 2.87\n",
            "epoch 70 training f1: 0.2461\n",
            "\n",
            "epoch 70 validation f1: 0.2505\n",
            "\n",
            "training time: 3.06\n",
            "epoch 71 training f1: 0.2462\n",
            "\n",
            "epoch 71 validation f1: 0.2570\n",
            "\n",
            "training time: 2.91\n",
            "epoch 72 training f1: 0.2561\n",
            "\n",
            "epoch 72 validation f1: 0.2472\n",
            "\n",
            "training time: 2.96\n",
            "epoch 73 training f1: 0.2566\n",
            "\n",
            "epoch 73 validation f1: 0.2596\n",
            "\n",
            "training time: 2.96\n",
            "epoch 74 training f1: 0.2664\n",
            "\n",
            "epoch 74 validation f1: 0.2546\n",
            "\n",
            "training time: 2.95\n",
            "epoch 75 training f1: 0.2633\n",
            "\n",
            "epoch 75 validation f1: 0.2597\n",
            "\n",
            "training time: 2.97\n",
            "epoch 76 training f1: 0.2693\n",
            "\n",
            "epoch 76 validation f1: 0.2518\n",
            "\n",
            "training time: 2.98\n",
            "epoch 77 training f1: 0.2605\n",
            "\n",
            "epoch 77 validation f1: 0.2581\n",
            "\n",
            "training time: 2.94\n",
            "epoch 78 training f1: 0.2699\n",
            "\n",
            "epoch 78 validation f1: 0.2624\n",
            "\n",
            "training time: 2.95\n",
            "epoch 79 training f1: 0.2631\n",
            "\n",
            "epoch 79 validation f1: 0.2626\n",
            "\n",
            "training time: 2.99\n",
            "epoch 80 training f1: 0.2777\n",
            "\n",
            "epoch 80 validation f1: 0.2588\n",
            "\n",
            "training time: 3.05\n",
            "epoch 81 training f1: 0.2901\n",
            "\n",
            "epoch 81 validation f1: 0.2681\n",
            "\n",
            "training time: 3.00\n",
            "epoch 82 training f1: 0.2894\n",
            "\n",
            "epoch 82 validation f1: 0.2671\n",
            "\n",
            "training time: 2.94\n",
            "epoch 83 training f1: 0.2902\n",
            "\n",
            "epoch 83 validation f1: 0.2667\n",
            "\n",
            "training time: 3.17\n",
            "epoch 84 training f1: 0.2846\n",
            "\n",
            "epoch 84 validation f1: 0.2634\n",
            "\n",
            "training time: 2.99\n",
            "epoch 85 training f1: 0.2908\n",
            "\n",
            "epoch 85 validation f1: 0.2551\n",
            "\n",
            "training time: 2.95\n",
            "epoch 86 training f1: 0.2875\n",
            "\n",
            "epoch 86 validation f1: 0.2560\n",
            "\n",
            "training time: 2.97\n",
            "epoch 87 training f1: 0.3044\n",
            "\n",
            "epoch 87 validation f1: 0.2547\n",
            "\n",
            "training time: 2.93\n",
            "epoch 88 training f1: 0.3024\n",
            "\n",
            "epoch 88 validation f1: 0.2513\n",
            "\n",
            "training time: 3.00\n",
            "epoch 89 training f1: 0.3075\n",
            "\n",
            "epoch 89 validation f1: 0.2558\n",
            "\n",
            "training time: 3.01\n",
            "epoch 90 training f1: 0.2987\n",
            "\n",
            "epoch 90 validation f1: 0.2668\n",
            "\n",
            "training time: 3.17\n",
            "epoch 91 training f1: 0.2968\n",
            "\n",
            "epoch 91 validation f1: 0.2677\n",
            "\n",
            "training time: 2.99\n",
            "epoch 92 training f1: 0.3118\n",
            "\n",
            "epoch 92 validation f1: 0.2604\n",
            "\n",
            "training time: 3.03\n",
            "epoch 93 training f1: 0.3068\n",
            "\n",
            "epoch 93 validation f1: 0.2635\n",
            "\n",
            "training time: 2.99\n",
            "epoch 94 training f1: 0.3105\n",
            "\n",
            "epoch 94 validation f1: 0.2652\n",
            "\n",
            "training time: 2.98\n",
            "epoch 95 training f1: 0.3266\n",
            "\n",
            "epoch 95 validation f1: 0.2647\n",
            "\n",
            "training time: 2.97\n",
            "epoch 96 training f1: 0.3205\n",
            "\n",
            "epoch 96 validation f1: 0.2606\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/BC2GM_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-01 04:38:29 tensorflow INFO: Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/BC2GM_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BC2GM 2000 no pretrain\n",
            "exact p: 0.2400\n",
            "exact r: 0.3038\n",
            "exact f: 0.2682\n",
            "partial p: 0.3931\n",
            "partial r: 0.4977\n",
            "partial f: 0.4393\n",
            "\n",
            "training BC4CHEMD 2000\n",
            "training network on 1600 documents, validation on 400 documents\n",
            "\n",
            "training time: 3.68\n",
            "epoch 1 training f1: 0.0387\n",
            "\n",
            "epoch 1 validation f1: 0.0372\n",
            "\n",
            "training time: 3.17\n",
            "epoch 2 training f1: 0.0344\n",
            "\n",
            "epoch 2 validation f1: 0.0321\n",
            "\n",
            "training time: 3.01\n",
            "epoch 3 training f1: 0.0253\n",
            "\n",
            "epoch 3 validation f1: 0.0103\n",
            "\n",
            "training time: 3.02\n",
            "epoch 4 training f1: 0.0028\n",
            "\n",
            "epoch 4 validation f1: 0.0000\n",
            "\n",
            "training time: 3.04\n",
            "epoch 5 training f1: 0.0000\n",
            "\n",
            "epoch 5 validation f1: 0.0000\n",
            "\n",
            "training time: 2.99\n",
            "epoch 6 training f1: 0.0015\n",
            "\n",
            "epoch 6 validation f1: 0.0000\n",
            "\n",
            "training time: 3.04\n",
            "epoch 7 training f1: 0.0015\n",
            "\n",
            "epoch 7 validation f1: 0.0000\n",
            "\n",
            "training time: 3.15\n",
            "epoch 8 training f1: 0.0029\n",
            "\n",
            "epoch 8 validation f1: 0.0066\n",
            "\n",
            "training time: 2.98\n",
            "epoch 9 training f1: 0.0086\n",
            "\n",
            "epoch 9 validation f1: 0.0261\n",
            "\n",
            "training time: 2.96\n",
            "epoch 10 training f1: 0.0126\n",
            "\n",
            "epoch 10 validation f1: 0.0256\n",
            "\n",
            "training time: 2.99\n",
            "epoch 11 training f1: 0.0154\n",
            "\n",
            "epoch 11 validation f1: 0.0255\n",
            "\n",
            "training time: 3.00\n",
            "epoch 12 training f1: 0.0126\n",
            "\n",
            "epoch 12 validation f1: 0.0252\n",
            "\n",
            "training time: 3.01\n",
            "epoch 13 training f1: 0.0195\n",
            "\n",
            "epoch 13 validation f1: 0.0251\n",
            "\n",
            "training time: 3.10\n",
            "epoch 14 training f1: 0.0084\n",
            "\n",
            "epoch 14 validation f1: 0.0310\n",
            "\n",
            "training time: 3.01\n",
            "epoch 15 training f1: 0.0223\n",
            "\n",
            "epoch 15 validation f1: 0.0370\n",
            "\n",
            "training time: 2.99\n",
            "epoch 16 training f1: 0.0386\n",
            "\n",
            "epoch 16 validation f1: 0.0370\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/BC4CHEMD_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-01 04:39:45 tensorflow INFO: Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/BC4CHEMD_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BC4CHEMD 2000 no pretrain\n",
            "exact p: 0.0209\n",
            "exact r: 0.2747\n",
            "exact f: 0.0388\n",
            "partial p: 0.0305\n",
            "partial r: 0.4012\n",
            "partial f: 0.0566\n",
            "\n",
            "training NCBI-disease 2000\n",
            "training network on 1600 documents, validation on 400 documents\n",
            "\n",
            "training time: 3.84\n",
            "epoch 1 training f1: 0.0305\n",
            "\n",
            "epoch 1 validation f1: 0.0318\n",
            "\n",
            "training time: 3.01\n",
            "epoch 2 training f1: 0.0291\n",
            "\n",
            "epoch 2 validation f1: 0.0227\n",
            "\n",
            "training time: 2.88\n",
            "epoch 3 training f1: 0.0266\n",
            "\n",
            "epoch 3 validation f1: 0.0290\n",
            "\n",
            "training time: 2.98\n",
            "epoch 4 training f1: 0.0186\n",
            "\n",
            "epoch 4 validation f1: 0.0092\n",
            "\n",
            "training time: 3.01\n",
            "epoch 5 training f1: 0.0097\n",
            "\n",
            "epoch 5 validation f1: 0.0089\n",
            "\n",
            "training time: 2.90\n",
            "epoch 6 training f1: 0.0078\n",
            "\n",
            "epoch 6 validation f1: 0.0091\n",
            "\n",
            "training time: 2.92\n",
            "epoch 7 training f1: 0.0078\n",
            "\n",
            "epoch 7 validation f1: 0.0135\n",
            "\n",
            "training time: 2.95\n",
            "epoch 8 training f1: 0.0143\n",
            "\n",
            "epoch 8 validation f1: 0.0181\n",
            "\n",
            "training time: 2.91\n",
            "epoch 9 training f1: 0.0152\n",
            "\n",
            "epoch 9 validation f1: 0.0181\n",
            "\n",
            "training time: 3.01\n",
            "epoch 10 training f1: 0.0201\n",
            "\n",
            "epoch 10 validation f1: 0.0219\n",
            "\n",
            "training time: 2.94\n",
            "epoch 11 training f1: 0.0197\n",
            "\n",
            "epoch 11 validation f1: 0.0257\n",
            "\n",
            "training time: 2.94\n",
            "epoch 12 training f1: 0.0269\n",
            "\n",
            "epoch 12 validation f1: 0.0407\n",
            "\n",
            "training time: 2.93\n",
            "epoch 13 training f1: 0.0313\n",
            "\n",
            "epoch 13 validation f1: 0.0659\n",
            "\n",
            "training time: 3.10\n",
            "epoch 14 training f1: 0.0430\n",
            "\n",
            "epoch 14 validation f1: 0.0816\n",
            "\n",
            "training time: 2.91\n",
            "epoch 15 training f1: 0.0528\n",
            "\n",
            "epoch 15 validation f1: 0.0976\n",
            "\n",
            "training time: 2.90\n",
            "epoch 16 training f1: 0.0638\n",
            "\n",
            "epoch 16 validation f1: 0.1014\n",
            "\n",
            "training time: 2.95\n",
            "epoch 17 training f1: 0.0773\n",
            "\n",
            "epoch 17 validation f1: 0.1156\n",
            "\n",
            "training time: 2.91\n",
            "epoch 18 training f1: 0.0852\n",
            "\n",
            "epoch 18 validation f1: 0.1291\n",
            "\n",
            "training time: 2.92\n",
            "epoch 19 training f1: 0.1021\n",
            "\n",
            "epoch 19 validation f1: 0.1501\n",
            "\n",
            "training time: 2.93\n",
            "epoch 20 training f1: 0.1139\n",
            "\n",
            "epoch 20 validation f1: 0.1648\n",
            "\n",
            "training time: 2.96\n",
            "epoch 21 training f1: 0.1357\n",
            "\n",
            "epoch 21 validation f1: 0.1710\n",
            "\n",
            "training time: 2.85\n",
            "epoch 22 training f1: 0.1448\n",
            "\n",
            "epoch 22 validation f1: 0.1896\n",
            "\n",
            "training time: 2.95\n",
            "epoch 23 training f1: 0.1777\n",
            "\n",
            "epoch 23 validation f1: 0.2078\n",
            "\n",
            "training time: 2.86\n",
            "epoch 24 training f1: 0.1851\n",
            "\n",
            "epoch 24 validation f1: 0.2151\n",
            "\n",
            "training time: 2.91\n",
            "epoch 25 training f1: 0.2053\n",
            "\n",
            "epoch 25 validation f1: 0.2179\n",
            "\n",
            "training time: 2.93\n",
            "epoch 26 training f1: 0.2102\n",
            "\n",
            "epoch 26 validation f1: 0.2266\n",
            "\n",
            "training time: 2.95\n",
            "epoch 27 training f1: 0.2364\n",
            "\n",
            "epoch 27 validation f1: 0.2370\n",
            "\n",
            "training time: 2.92\n",
            "epoch 28 training f1: 0.2404\n",
            "\n",
            "epoch 28 validation f1: 0.2578\n",
            "\n",
            "training time: 2.94\n",
            "epoch 29 training f1: 0.2604\n",
            "\n",
            "epoch 29 validation f1: 0.2690\n",
            "\n",
            "training time: 2.90\n",
            "epoch 30 training f1: 0.2729\n",
            "\n",
            "epoch 30 validation f1: 0.2720\n",
            "\n",
            "training time: 2.92\n",
            "epoch 31 training f1: 0.2952\n",
            "\n",
            "epoch 31 validation f1: 0.2760\n",
            "\n",
            "training time: 2.92\n",
            "epoch 32 training f1: 0.3037\n",
            "\n",
            "epoch 32 validation f1: 0.2865\n",
            "\n",
            "training time: 2.88\n",
            "epoch 33 training f1: 0.3140\n",
            "\n",
            "epoch 33 validation f1: 0.3028\n",
            "\n",
            "training time: 2.95\n",
            "epoch 34 training f1: 0.3345\n",
            "\n",
            "epoch 34 validation f1: 0.3086\n",
            "\n",
            "training time: 2.98\n",
            "epoch 35 training f1: 0.3659\n",
            "\n",
            "epoch 35 validation f1: 0.3130\n",
            "\n",
            "training time: 3.12\n",
            "epoch 36 training f1: 0.3656\n",
            "\n",
            "epoch 36 validation f1: 0.3525\n",
            "\n",
            "training time: 2.90\n",
            "epoch 37 training f1: 0.3829\n",
            "\n",
            "epoch 37 validation f1: 0.3568\n",
            "\n",
            "training time: 2.88\n",
            "epoch 38 training f1: 0.3896\n",
            "\n",
            "epoch 38 validation f1: 0.3782\n",
            "\n",
            "training time: 2.93\n",
            "epoch 39 training f1: 0.3873\n",
            "\n",
            "epoch 39 validation f1: 0.3872\n",
            "\n",
            "training time: 2.95\n",
            "epoch 40 training f1: 0.3996\n",
            "\n",
            "epoch 40 validation f1: 0.3721\n",
            "\n",
            "training time: 2.94\n",
            "epoch 41 training f1: 0.4093\n",
            "\n",
            "epoch 41 validation f1: 0.3952\n",
            "\n",
            "training time: 3.01\n",
            "epoch 42 training f1: 0.4059\n",
            "\n",
            "epoch 42 validation f1: 0.3897\n",
            "\n",
            "training time: 3.10\n",
            "epoch 43 training f1: 0.4170\n",
            "\n",
            "epoch 43 validation f1: 0.3989\n",
            "\n",
            "training time: 2.96\n",
            "epoch 44 training f1: 0.4280\n",
            "\n",
            "epoch 44 validation f1: 0.4114\n",
            "\n",
            "training time: 2.97\n",
            "epoch 45 training f1: 0.4335\n",
            "\n",
            "epoch 45 validation f1: 0.4069\n",
            "\n",
            "training time: 2.96\n",
            "epoch 46 training f1: 0.4429\n",
            "\n",
            "epoch 46 validation f1: 0.4122\n",
            "\n",
            "training time: 3.04\n",
            "epoch 47 training f1: 0.4583\n",
            "\n",
            "epoch 47 validation f1: 0.4143\n",
            "\n",
            "training time: 2.96\n",
            "epoch 48 training f1: 0.4484\n",
            "\n",
            "epoch 48 validation f1: 0.4168\n",
            "\n",
            "training time: 2.97\n",
            "epoch 49 training f1: 0.4534\n",
            "\n",
            "epoch 49 validation f1: 0.4178\n",
            "\n",
            "training time: 2.99\n",
            "epoch 50 training f1: 0.4793\n",
            "\n",
            "epoch 50 validation f1: 0.4162\n",
            "\n",
            "training time: 2.90\n",
            "epoch 51 training f1: 0.4827\n",
            "\n",
            "epoch 51 validation f1: 0.4216\n",
            "\n",
            "training time: 2.91\n",
            "epoch 52 training f1: 0.4679\n",
            "\n",
            "epoch 52 validation f1: 0.4257\n",
            "\n",
            "training time: 2.87\n",
            "epoch 53 training f1: 0.4883\n",
            "\n",
            "epoch 53 validation f1: 0.4189\n",
            "\n",
            "training time: 2.93\n",
            "epoch 54 training f1: 0.4847\n",
            "\n",
            "epoch 54 validation f1: 0.4327\n",
            "\n",
            "training time: 2.97\n",
            "epoch 55 training f1: 0.4995\n",
            "\n",
            "epoch 55 validation f1: 0.4368\n",
            "\n",
            "training time: 2.90\n",
            "epoch 56 training f1: 0.4917\n",
            "\n",
            "epoch 56 validation f1: 0.4392\n",
            "\n",
            "training time: 2.97\n",
            "epoch 57 training f1: 0.4915\n",
            "\n",
            "epoch 57 validation f1: 0.4377\n",
            "\n",
            "training time: 2.94\n",
            "epoch 58 training f1: 0.5011\n",
            "\n",
            "epoch 58 validation f1: 0.4386\n",
            "\n",
            "training time: 2.92\n",
            "epoch 59 training f1: 0.5054\n",
            "\n",
            "epoch 59 validation f1: 0.4538\n",
            "\n",
            "training time: 2.93\n",
            "epoch 60 training f1: 0.5195\n",
            "\n",
            "epoch 60 validation f1: 0.4597\n",
            "\n",
            "training time: 2.95\n",
            "epoch 61 training f1: 0.5213\n",
            "\n",
            "epoch 61 validation f1: 0.4548\n",
            "\n",
            "training time: 2.96\n",
            "epoch 62 training f1: 0.5362\n",
            "\n",
            "epoch 62 validation f1: 0.4643\n",
            "\n",
            "training time: 2.94\n",
            "epoch 63 training f1: 0.5273\n",
            "\n",
            "epoch 63 validation f1: 0.4668\n",
            "\n",
            "training time: 2.91\n",
            "epoch 64 training f1: 0.5294\n",
            "\n",
            "epoch 64 validation f1: 0.4695\n",
            "\n",
            "training time: 2.92\n",
            "epoch 65 training f1: 0.5412\n",
            "\n",
            "epoch 65 validation f1: 0.4736\n",
            "\n",
            "training time: 2.93\n",
            "epoch 66 training f1: 0.5468\n",
            "\n",
            "epoch 66 validation f1: 0.4850\n",
            "\n",
            "training time: 2.89\n",
            "epoch 67 training f1: 0.5472\n",
            "\n",
            "epoch 67 validation f1: 0.4659\n",
            "\n",
            "training time: 2.98\n",
            "epoch 68 training f1: 0.5484\n",
            "\n",
            "epoch 68 validation f1: 0.4799\n",
            "\n",
            "training time: 2.95\n",
            "epoch 69 training f1: 0.5508\n",
            "\n",
            "epoch 69 validation f1: 0.4794\n",
            "\n",
            "training time: 2.91\n",
            "epoch 70 training f1: 0.5622\n",
            "\n",
            "epoch 70 validation f1: 0.4821\n",
            "\n",
            "training time: 2.91\n",
            "epoch 71 training f1: 0.5686\n",
            "\n",
            "epoch 71 validation f1: 0.4887\n",
            "\n",
            "training time: 3.06\n",
            "epoch 72 training f1: 0.5659\n",
            "\n",
            "epoch 72 validation f1: 0.4960\n",
            "\n",
            "training time: 2.93\n",
            "epoch 73 training f1: 0.5727\n",
            "\n",
            "epoch 73 validation f1: 0.4940\n",
            "\n",
            "training time: 2.95\n",
            "epoch 74 training f1: 0.5825\n",
            "\n",
            "epoch 74 validation f1: 0.4933\n",
            "\n",
            "training time: 2.92\n",
            "epoch 75 training f1: 0.5788\n",
            "\n",
            "epoch 75 validation f1: 0.5000\n",
            "\n",
            "training time: 2.92\n",
            "epoch 76 training f1: 0.5809\n",
            "\n",
            "epoch 76 validation f1: 0.4861\n",
            "\n",
            "training time: 2.92\n",
            "epoch 77 training f1: 0.5851\n",
            "\n",
            "epoch 77 validation f1: 0.4907\n",
            "\n",
            "training time: 2.96\n",
            "epoch 78 training f1: 0.6030\n",
            "\n",
            "epoch 78 validation f1: 0.4960\n",
            "\n",
            "training time: 2.95\n",
            "epoch 79 training f1: 0.5910\n",
            "\n",
            "epoch 79 validation f1: 0.4980\n",
            "\n",
            "training time: 2.89\n",
            "epoch 80 training f1: 0.6001\n",
            "\n",
            "epoch 80 validation f1: 0.4954\n",
            "\n",
            "training time: 2.87\n",
            "epoch 81 training f1: 0.6068\n",
            "\n",
            "epoch 81 validation f1: 0.4948\n",
            "\n",
            "training time: 2.91\n",
            "epoch 82 training f1: 0.6077\n",
            "\n",
            "epoch 82 validation f1: 0.5000\n",
            "\n",
            "training time: 2.92\n",
            "epoch 83 training f1: 0.6144\n",
            "\n",
            "epoch 83 validation f1: 0.5000\n",
            "\n",
            "training time: 2.92\n",
            "epoch 84 training f1: 0.6168\n",
            "\n",
            "epoch 84 validation f1: 0.4973\n",
            "\n",
            "training time: 2.92\n",
            "epoch 85 training f1: 0.6237\n",
            "\n",
            "epoch 85 validation f1: 0.5053\n",
            "\n",
            "training time: 2.96\n",
            "epoch 86 training f1: 0.6320\n",
            "\n",
            "epoch 86 validation f1: 0.4993\n",
            "\n",
            "training time: 3.10\n",
            "epoch 87 training f1: 0.6125\n",
            "\n",
            "epoch 87 validation f1: 0.4935\n",
            "\n",
            "training time: 2.99\n",
            "epoch 88 training f1: 0.6345\n",
            "\n",
            "epoch 88 validation f1: 0.4921\n",
            "\n",
            "training time: 2.93\n",
            "epoch 89 training f1: 0.6366\n",
            "\n",
            "epoch 89 validation f1: 0.5034\n",
            "\n",
            "training time: 2.90\n",
            "epoch 90 training f1: 0.6236\n",
            "\n",
            "epoch 90 validation f1: 0.4954\n",
            "\n",
            "training time: 2.91\n",
            "epoch 91 training f1: 0.6425\n",
            "\n",
            "epoch 91 validation f1: 0.4856\n",
            "\n",
            "training time: 2.94\n",
            "epoch 92 training f1: 0.6438\n",
            "\n",
            "epoch 92 validation f1: 0.4915\n",
            "\n",
            "training time: 2.98\n",
            "epoch 93 training f1: 0.6576\n",
            "\n",
            "epoch 93 validation f1: 0.4902\n",
            "\n",
            "training time: 2.96\n",
            "epoch 94 training f1: 0.6453\n",
            "\n",
            "epoch 94 validation f1: 0.4942\n",
            "\n",
            "training time: 2.93\n",
            "epoch 95 training f1: 0.6417\n",
            "\n",
            "epoch 95 validation f1: 0.4993\n",
            "\n",
            "training time: 2.94\n",
            "epoch 96 training f1: 0.6482\n",
            "\n",
            "epoch 96 validation f1: 0.5060\n",
            "\n",
            "training time: 2.92\n",
            "epoch 97 training f1: 0.6595\n",
            "\n",
            "epoch 97 validation f1: 0.5040\n",
            "\n",
            "training time: 2.92\n",
            "epoch 98 training f1: 0.6718\n",
            "\n",
            "epoch 98 validation f1: 0.5108\n",
            "\n",
            "training time: 2.95\n",
            "epoch 99 training f1: 0.6763\n",
            "\n",
            "epoch 99 validation f1: 0.5101\n",
            "\n",
            "training time: 2.98\n",
            "epoch 100 training f1: 0.6825\n",
            "\n",
            "epoch 100 validation f1: 0.5073\n",
            "\n",
            "training time: 2.95\n",
            "epoch 101 training f1: 0.6914\n",
            "\n",
            "epoch 101 validation f1: 0.5223\n",
            "\n",
            "training time: 2.95\n",
            "epoch 102 training f1: 0.6896\n",
            "\n",
            "epoch 102 validation f1: 0.5245\n",
            "\n",
            "training time: 2.99\n",
            "epoch 103 training f1: 0.6786\n",
            "\n",
            "epoch 103 validation f1: 0.5141\n",
            "\n",
            "training time: 2.97\n",
            "epoch 104 training f1: 0.6980\n",
            "\n",
            "epoch 104 validation f1: 0.5220\n",
            "\n",
            "training time: 2.94\n",
            "epoch 105 training f1: 0.6976\n",
            "\n",
            "epoch 105 validation f1: 0.5129\n",
            "\n",
            "training time: 2.97\n",
            "epoch 106 training f1: 0.7009\n",
            "\n",
            "epoch 106 validation f1: 0.5230\n",
            "\n",
            "training time: 2.96\n",
            "epoch 107 training f1: 0.7080\n",
            "\n",
            "epoch 107 validation f1: 0.5186\n",
            "\n",
            "training time: 2.93\n",
            "epoch 108 training f1: 0.6999\n",
            "\n",
            "epoch 108 validation f1: 0.5147\n",
            "\n",
            "training time: 2.99\n",
            "epoch 109 training f1: 0.6976\n",
            "\n",
            "epoch 109 validation f1: 0.5259\n",
            "\n",
            "training time: 2.97\n",
            "epoch 110 training f1: 0.7237\n",
            "\n",
            "epoch 110 validation f1: 0.5315\n",
            "\n",
            "training time: 2.94\n",
            "epoch 111 training f1: 0.7020\n",
            "\n",
            "epoch 111 validation f1: 0.5244\n",
            "\n",
            "training time: 3.06\n",
            "epoch 112 training f1: 0.7187\n",
            "\n",
            "epoch 112 validation f1: 0.5319\n",
            "\n",
            "training time: 2.91\n",
            "epoch 113 training f1: 0.7141\n",
            "\n",
            "epoch 113 validation f1: 0.5433\n",
            "\n",
            "training time: 2.92\n",
            "epoch 114 training f1: 0.7229\n",
            "\n",
            "epoch 114 validation f1: 0.5302\n",
            "\n",
            "training time: 2.94\n",
            "epoch 115 training f1: 0.7269\n",
            "\n",
            "epoch 115 validation f1: 0.5294\n",
            "\n",
            "training time: 3.10\n",
            "epoch 116 training f1: 0.7179\n",
            "\n",
            "epoch 116 validation f1: 0.5438\n",
            "\n",
            "training time: 2.97\n",
            "epoch 117 training f1: 0.7306\n",
            "\n",
            "epoch 117 validation f1: 0.5328\n",
            "\n",
            "training time: 2.93\n",
            "epoch 118 training f1: 0.7404\n",
            "\n",
            "epoch 118 validation f1: 0.5291\n",
            "\n",
            "training time: 2.95\n",
            "epoch 119 training f1: 0.7506\n",
            "\n",
            "epoch 119 validation f1: 0.5443\n",
            "\n",
            "training time: 2.95\n",
            "epoch 120 training f1: 0.7487\n",
            "\n",
            "epoch 120 validation f1: 0.5372\n",
            "\n",
            "training time: 2.92\n",
            "epoch 121 training f1: 0.7524\n",
            "\n",
            "epoch 121 validation f1: 0.5507\n",
            "\n",
            "training time: 2.90\n",
            "epoch 122 training f1: 0.7517\n",
            "\n",
            "epoch 122 validation f1: 0.5348\n",
            "\n",
            "training time: 2.98\n",
            "epoch 123 training f1: 0.7449\n",
            "\n",
            "epoch 123 validation f1: 0.5540\n",
            "\n",
            "training time: 3.01\n",
            "epoch 124 training f1: 0.7556\n",
            "\n",
            "epoch 124 validation f1: 0.5443\n",
            "\n",
            "training time: 2.95\n",
            "epoch 125 training f1: 0.7615\n",
            "\n",
            "epoch 125 validation f1: 0.5503\n",
            "\n",
            "training time: 3.01\n",
            "epoch 126 training f1: 0.7576\n",
            "\n",
            "epoch 126 validation f1: 0.5476\n",
            "\n",
            "training time: 2.96\n",
            "epoch 127 training f1: 0.7591\n",
            "\n",
            "epoch 127 validation f1: 0.5526\n",
            "\n",
            "training time: 2.94\n",
            "epoch 128 training f1: 0.7629\n",
            "\n",
            "epoch 128 validation f1: 0.5360\n",
            "\n",
            "training time: 2.88\n",
            "epoch 129 training f1: 0.7615\n",
            "\n",
            "epoch 129 validation f1: 0.5423\n",
            "\n",
            "training time: 2.93\n",
            "epoch 130 training f1: 0.7673\n",
            "\n",
            "epoch 130 validation f1: 0.5564\n",
            "\n",
            "training time: 2.93\n",
            "epoch 131 training f1: 0.7756\n",
            "\n",
            "epoch 131 validation f1: 0.5452\n",
            "\n",
            "training time: 2.93\n",
            "epoch 132 training f1: 0.7940\n",
            "\n",
            "epoch 132 validation f1: 0.5402\n",
            "\n",
            "training time: 2.99\n",
            "epoch 133 training f1: 0.7832\n",
            "\n",
            "epoch 133 validation f1: 0.5416\n",
            "\n",
            "training time: 2.97\n",
            "epoch 134 training f1: 0.7752\n",
            "\n",
            "epoch 134 validation f1: 0.5414\n",
            "\n",
            "training time: 2.96\n",
            "epoch 135 training f1: 0.7932\n",
            "\n",
            "epoch 135 validation f1: 0.5457\n",
            "\n",
            "training time: 2.97\n",
            "epoch 136 training f1: 0.7823\n",
            "\n",
            "epoch 136 validation f1: 0.5587\n",
            "\n",
            "training time: 2.93\n",
            "epoch 137 training f1: 0.7934\n",
            "\n",
            "epoch 137 validation f1: 0.5534\n",
            "\n",
            "training time: 2.95\n",
            "epoch 138 training f1: 0.8014\n",
            "\n",
            "epoch 138 validation f1: 0.5421\n",
            "\n",
            "training time: 2.95\n",
            "epoch 139 training f1: 0.7983\n",
            "\n",
            "epoch 139 validation f1: 0.5401\n",
            "\n",
            "training time: 2.97\n",
            "epoch 140 training f1: 0.7864\n",
            "\n",
            "epoch 140 validation f1: 0.5457\n",
            "\n",
            "training time: 2.94\n",
            "epoch 141 training f1: 0.7987\n",
            "\n",
            "epoch 141 validation f1: 0.5394\n",
            "\n",
            "training time: 2.94\n",
            "epoch 142 training f1: 0.7982\n",
            "\n",
            "epoch 142 validation f1: 0.5418\n",
            "\n",
            "training time: 2.89\n",
            "epoch 143 training f1: 0.7961\n",
            "\n",
            "epoch 143 validation f1: 0.5464\n",
            "\n",
            "training time: 2.93\n",
            "epoch 144 training f1: 0.8169\n",
            "\n",
            "epoch 144 validation f1: 0.5440\n",
            "\n",
            "training time: 2.92\n",
            "epoch 145 training f1: 0.8194\n",
            "\n",
            "epoch 145 validation f1: 0.5393\n",
            "\n",
            "training time: 3.09\n",
            "epoch 146 training f1: 0.8173\n",
            "\n",
            "epoch 146 validation f1: 0.5335\n",
            "\n",
            "training time: 2.89\n",
            "epoch 147 training f1: 0.8147\n",
            "\n",
            "epoch 147 validation f1: 0.5493\n",
            "\n",
            "training time: 2.94\n",
            "epoch 148 training f1: 0.8141\n",
            "\n",
            "epoch 148 validation f1: 0.5491\n",
            "\n",
            "training time: 2.94\n",
            "epoch 149 training f1: 0.8131\n",
            "\n",
            "epoch 149 validation f1: 0.5534\n",
            "\n",
            "training time: 2.92\n",
            "epoch 150 training f1: 0.8126\n",
            "\n",
            "epoch 150 validation f1: 0.5471\n",
            "\n",
            "training time: 2.90\n",
            "epoch 151 training f1: 0.8186\n",
            "\n",
            "epoch 151 validation f1: 0.5608\n",
            "\n",
            "training time: 2.93\n",
            "epoch 152 training f1: 0.8373\n",
            "\n",
            "epoch 152 validation f1: 0.5409\n",
            "\n",
            "training time: 2.95\n",
            "epoch 153 training f1: 0.8300\n",
            "\n",
            "epoch 153 validation f1: 0.5512\n",
            "\n",
            "training time: 2.97\n",
            "epoch 154 training f1: 0.8384\n",
            "\n",
            "epoch 154 validation f1: 0.5506\n",
            "\n",
            "training time: 2.92\n",
            "epoch 155 training f1: 0.8416\n",
            "\n",
            "epoch 155 validation f1: 0.5418\n",
            "\n",
            "training time: 2.97\n",
            "epoch 156 training f1: 0.8395\n",
            "\n",
            "epoch 156 validation f1: 0.5471\n",
            "\n",
            "training time: 2.88\n",
            "epoch 157 training f1: 0.8454\n",
            "\n",
            "epoch 157 validation f1: 0.5488\n",
            "\n",
            "training time: 2.93\n",
            "epoch 158 training f1: 0.8339\n",
            "\n",
            "epoch 158 validation f1: 0.5523\n",
            "\n",
            "training time: 2.95\n",
            "epoch 159 training f1: 0.8423\n",
            "\n",
            "epoch 159 validation f1: 0.5491\n",
            "\n",
            "training time: 3.09\n",
            "epoch 160 training f1: 0.8511\n",
            "\n",
            "epoch 160 validation f1: 0.5630\n",
            "\n",
            "training time: 2.99\n",
            "epoch 161 training f1: 0.8359\n",
            "\n",
            "epoch 161 validation f1: 0.5462\n",
            "\n",
            "training time: 2.96\n",
            "epoch 162 training f1: 0.8491\n",
            "\n",
            "epoch 162 validation f1: 0.5445\n",
            "\n",
            "training time: 2.92\n",
            "epoch 163 training f1: 0.8547\n",
            "\n",
            "epoch 163 validation f1: 0.5513\n",
            "\n",
            "training time: 3.06\n",
            "epoch 164 training f1: 0.8393\n",
            "\n",
            "epoch 164 validation f1: 0.5402\n",
            "\n",
            "training time: 2.93\n",
            "epoch 165 training f1: 0.8588\n",
            "\n",
            "epoch 165 validation f1: 0.5501\n",
            "\n",
            "training time: 2.95\n",
            "epoch 166 training f1: 0.8485\n",
            "\n",
            "epoch 166 validation f1: 0.5559\n",
            "\n",
            "training time: 2.97\n",
            "epoch 167 training f1: 0.8581\n",
            "\n",
            "epoch 167 validation f1: 0.5523\n",
            "\n",
            "training time: 2.96\n",
            "epoch 168 training f1: 0.8694\n",
            "\n",
            "epoch 168 validation f1: 0.5601\n",
            "\n",
            "training time: 2.94\n",
            "epoch 169 training f1: 0.8657\n",
            "\n",
            "epoch 169 validation f1: 0.5699\n",
            "\n",
            "training time: 2.92\n",
            "epoch 170 training f1: 0.8643\n",
            "\n",
            "epoch 170 validation f1: 0.5544\n",
            "\n",
            "training time: 2.96\n",
            "epoch 171 training f1: 0.8670\n",
            "\n",
            "epoch 171 validation f1: 0.5524\n",
            "\n",
            "training time: 2.96\n",
            "epoch 172 training f1: 0.8530\n",
            "\n",
            "epoch 172 validation f1: 0.5628\n",
            "\n",
            "training time: 2.94\n",
            "epoch 173 training f1: 0.8634\n",
            "\n",
            "epoch 173 validation f1: 0.5636\n",
            "\n",
            "training time: 2.93\n",
            "epoch 174 training f1: 0.8652\n",
            "\n",
            "epoch 174 validation f1: 0.5496\n",
            "\n",
            "training time: 2.96\n",
            "epoch 175 training f1: 0.8764\n",
            "\n",
            "epoch 175 validation f1: 0.5800\n",
            "\n",
            "training time: 3.10\n",
            "epoch 176 training f1: 0.8652\n",
            "\n",
            "epoch 176 validation f1: 0.5759\n",
            "\n",
            "training time: 2.93\n",
            "epoch 177 training f1: 0.8804\n",
            "\n",
            "epoch 177 validation f1: 0.5662\n",
            "\n",
            "training time: 2.98\n",
            "epoch 178 training f1: 0.8872\n",
            "\n",
            "epoch 178 validation f1: 0.5643\n",
            "\n",
            "training time: 2.97\n",
            "epoch 179 training f1: 0.8848\n",
            "\n",
            "epoch 179 validation f1: 0.5642\n",
            "\n",
            "training time: 2.96\n",
            "epoch 180 training f1: 0.8752\n",
            "\n",
            "epoch 180 validation f1: 0.5631\n",
            "\n",
            "training time: 2.96\n",
            "epoch 181 training f1: 0.8740\n",
            "\n",
            "epoch 181 validation f1: 0.5622\n",
            "\n",
            "training time: 2.95\n",
            "epoch 182 training f1: 0.8794\n",
            "\n",
            "epoch 182 validation f1: 0.5631\n",
            "\n",
            "training time: 3.15\n",
            "epoch 183 training f1: 0.8763\n",
            "\n",
            "epoch 183 validation f1: 0.5625\n",
            "\n",
            "training time: 2.94\n",
            "epoch 184 training f1: 0.8815\n",
            "\n",
            "epoch 184 validation f1: 0.5583\n",
            "\n",
            "training time: 2.94\n",
            "epoch 185 training f1: 0.8839\n",
            "\n",
            "epoch 185 validation f1: 0.5639\n",
            "\n",
            "training time: 2.96\n",
            "epoch 186 training f1: 0.8727\n",
            "\n",
            "epoch 186 validation f1: 0.5563\n",
            "\n",
            "training time: 2.95\n",
            "epoch 187 training f1: 0.8880\n",
            "\n",
            "epoch 187 validation f1: 0.5744\n",
            "\n",
            "training time: 2.93\n",
            "epoch 188 training f1: 0.8943\n",
            "\n",
            "epoch 188 validation f1: 0.5501\n",
            "\n",
            "training time: 2.94\n",
            "epoch 189 training f1: 0.8905\n",
            "\n",
            "epoch 189 validation f1: 0.5777\n",
            "\n",
            "training time: 3.10\n",
            "epoch 190 training f1: 0.8884\n",
            "\n",
            "epoch 190 validation f1: 0.5615\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/NCBI-disease_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-01 04:52:44 tensorflow INFO: Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/NCBI-disease_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NCBI-disease 2000 no pretrain\n",
            "exact p: 0.5386\n",
            "exact r: 0.4937\n",
            "exact f: 0.5152\n",
            "partial p: 0.6569\n",
            "partial r: 0.6020\n",
            "partial f: 0.6282\n",
            "\n",
            "training s800 2000\n",
            "training network on 1600 documents, validation on 400 documents\n",
            "\n",
            "training time: 3.61\n",
            "epoch 1 training f1: 0.0083\n",
            "\n",
            "epoch 1 validation f1: 0.0096\n",
            "\n",
            "training time: 2.94\n",
            "epoch 2 training f1: 0.0085\n",
            "\n",
            "epoch 2 validation f1: 0.0159\n",
            "\n",
            "training time: 3.11\n",
            "epoch 3 training f1: 0.0091\n",
            "\n",
            "epoch 3 validation f1: 0.0120\n",
            "\n",
            "training time: 2.98\n",
            "epoch 4 training f1: 0.0076\n",
            "\n",
            "epoch 4 validation f1: 0.0000\n",
            "\n",
            "training time: 2.99\n",
            "epoch 5 training f1: 0.0000\n",
            "\n",
            "epoch 5 validation f1: 0.0000\n",
            "\n",
            "training time: 2.99\n",
            "epoch 6 training f1: 0.0022\n",
            "\n",
            "epoch 6 validation f1: 0.0000\n",
            "\n",
            "training time: 2.95\n",
            "epoch 7 training f1: 0.0000\n",
            "\n",
            "epoch 7 validation f1: 0.0000\n",
            "\n",
            "training time: 2.96\n",
            "epoch 8 training f1: 0.0020\n",
            "\n",
            "epoch 8 validation f1: 0.0000\n",
            "\n",
            "training time: 2.90\n",
            "epoch 9 training f1: 0.0000\n",
            "\n",
            "epoch 9 validation f1: 0.0000\n",
            "\n",
            "training time: 2.96\n",
            "epoch 10 training f1: 0.0020\n",
            "\n",
            "epoch 10 validation f1: 0.0000\n",
            "\n",
            "training time: 3.01\n",
            "epoch 11 training f1: 0.0019\n",
            "\n",
            "epoch 11 validation f1: 0.0000\n",
            "\n",
            "training time: 2.93\n",
            "epoch 12 training f1: 0.0019\n",
            "\n",
            "epoch 12 validation f1: 0.0000\n",
            "\n",
            "training time: 2.97\n",
            "epoch 13 training f1: 0.0000\n",
            "\n",
            "epoch 13 validation f1: 0.0000\n",
            "\n",
            "training time: 2.96\n",
            "epoch 14 training f1: 0.0037\n",
            "\n",
            "epoch 14 validation f1: 0.0000\n",
            "\n",
            "training time: 2.98\n",
            "epoch 15 training f1: 0.0036\n",
            "\n",
            "epoch 15 validation f1: 0.0000\n",
            "\n",
            "training time: 2.92\n",
            "epoch 16 training f1: 0.0036\n",
            "\n",
            "epoch 16 validation f1: 0.0000\n",
            "\n",
            "training time: 2.93\n",
            "epoch 17 training f1: 0.0036\n",
            "\n",
            "epoch 17 validation f1: 0.0000\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/s800_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-01 04:53:57 tensorflow INFO: Restoring parameters from /content/drive/MyDrive/Pubmed/FinetuneNotSemisup/s800_lstm_crf_medmentions_1600.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "s800 2000 no pretrain\n",
            "exact p: 0.0023\n",
            "exact r: 0.0293\n",
            "exact f: 0.0042\n",
            "partial p: 0.0076\n",
            "partial r: 0.0987\n",
            "partial f: 0.0141\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for dataset in datasets:\n",
        "\n",
        "    # iterate over each data size setting\n",
        "    for data_size in data_sizes:\n",
        "\n",
        "        print('training',dataset,data_size)\n",
        "        \n",
        "        # load data\n",
        "        X = np.load('/content/drive/MyDrive/Pubmed/%s_X_train.npy' % dataset).astype(np.int32)[:data_size]\n",
        "        y = np.load('/content/drive/MyDrive/Pubmed/%s_y_train.npy' % dataset).astype(np.int32)[:data_size]\n",
        "        max_len = 50\n",
        "        num_classes = np.max(y) + 1\n",
        "        \n",
        "        with open('/content/drive/MyDrive/Pubmed/%s_senlens_train.pkl' % dataset,'rb') as f:\n",
        "            doc_len = pickle.load(f)[:data_size]\n",
        "        doc_len = [l if l <=max_len else max_len for l in doc_len]\n",
        "        \n",
        "        # load conversion dictionaries\n",
        "        label2idx = {'O':0, 'B':1, 'I': 2}\n",
        "        idx2label = {0:'O',1:'B-ENT',2:'I-ENT'}\n",
        "            \n",
        "        # train val split\n",
        "        num_docs = len(X)\n",
        "        train_size = int(num_docs * 0.8)\n",
        "        X_train = X[:train_size]\n",
        "        X_val = X[train_size:]\n",
        "        y_train = y[:train_size]\n",
        "        y_val = y[train_size:]\n",
        "        doc_len_train = doc_len[:train_size]\n",
        "        doc_len_val = doc_len[train_size:]\n",
        "        \n",
        "        # load test data\n",
        "        X_test = np.load('/content/drive/MyDrive/Pubmed/%s_X_test.npy' % dataset).astype(np.int32)\n",
        "        y_test = np.load('/content/drive/MyDrive/Pubmed/%s_y_test.npy' % dataset).astype(np.int32)\n",
        "        with open('/content/drive/MyDrive/Pubmed/%s_senlens_test.pkl' % dataset,'rb') as f:\n",
        "            doc_len_test = pickle.load(f)\n",
        "        doc_len_test = [l if l <=max_len else max_len for l in doc_len_test]\n",
        "        y_true = []\n",
        "        for y_true_,l in zip(y_test,doc_len_test):\n",
        "            y = [idx2label[l] for l in y_true_[:l]]\n",
        "            y_true.append(y)\n",
        "        \n",
        "\n",
        "        # train model from scratch\n",
        "        tf.reset_default_graph()\n",
        "        model = lstm_crf(vocab,num_classes,max_len)\n",
        "        \n",
        "        model.train(X_train,y_train,doc_len_train,\n",
        "                    batch_size=batch_size_f,patience=patience_f,\n",
        "                    validation_data=(X_val,y_val,doc_len_val),\n",
        "                    savebest=True,filepath='/content/drive/MyDrive/Pubmed/FinetuneNotSemisup/%s_lstm_crf_%s_%i.ckpt' % (dataset,pretrain_dataset,train_size))\n",
        "        \n",
        "        # evaluate on test set\n",
        "        model.load('/content/drive/MyDrive/Pubmed/FinetuneNotSemisup/%s_lstm_crf_%s_%i.ckpt' % (dataset,pretrain_dataset,train_size))\n",
        "        y_preds_,_,_ = model.predict(X_test,doc_len_test)\n",
        "        y_pred = []\n",
        "        for y_pred_,l in zip(y_preds_,doc_len_test):\n",
        "            y = [idx2label[l] for l in y_pred_[:l]]\n",
        "            y_pred.append(y)\n",
        "        \n",
        "        evaluator = Evaluator(y_true, y_pred, tags=['ENT'], loader=\"list\")\n",
        "        results, results_by_tag = evaluator.evaluate()\n",
        "        \n",
        "        exact_p = results['exact']['precision']\n",
        "        exact_r = results['exact']['recall']\n",
        "        exact_f = results['exact']['f1']\n",
        "        partial_p = results['partial']['precision']\n",
        "        partial_r = results['partial']['recall']\n",
        "        partial_f = results['partial']['f1']\n",
        "    \n",
        "        print(dataset, data_size, 'no pretrain')\n",
        "        print('exact p: %.4f' % exact_p)\n",
        "        print('exact r: %.4f' % exact_r)\n",
        "        print('exact f: %.4f' % exact_f)\n",
        "        print('partial p: %.4f' % partial_p)\n",
        "        print('partial r: %.4f' % partial_r)\n",
        "        print('partial f: %.4f' % partial_f)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNY96JO1M3IM"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLzjCHkMM3Jc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLrYqEM_M3Mf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI9-qNQWM3OJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GcwDoc7M3Q-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6dV5uU6M3SR"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6R5CevmM3Vq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sg9IDoTCM3YD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVdNucNXM3aE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEiELLyTM3cH"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wO-ArQLM3eb"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PhjrVX8M3gC"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g53syWfsM3i9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8IUi9KjM3kh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfnxluusM3np"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7iJ07QuM3pN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uKmKoC-M3sD"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZmZADgPM3tw"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSyfzrQdM3ws"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lrpzd97M3yK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vh34b13OM31L"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcrW7065M32v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8arhnY_M35v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucw50RQ6M37W"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6EweK8UM3-b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqOaTi0uM4AA"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2LqMpTxM4DK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr8FjQ9sM4Eo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhomU0ufM4IJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BiLSTM_CRF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}